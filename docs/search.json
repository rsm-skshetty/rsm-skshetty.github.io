[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Samiksha Shetty",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the early 2000s, economists Dean Karlan and John List partnered with a politically oriented nonprofit organization to run a large-scale natural field experiment on charitable giving. The organization sent out over 50,000 fundraising letters to prior donors, randomly assigning each recipient to a different version of the letter.\nThe central research question was: Does offering a matching grant increase the likelihood and amount of charitable donations?\nTo answer this, recipients were randomly assigned to: - A control group that received a standard appeal, - A treatment group that was told a generous donor would match their gift at one of several ratios: 1:1, 2:1, or 3:1.\nOther variables were randomized as well: - The maximum amount of the matching grant ($25k, $50k, $100k, or unstated), - The suggested donation amount, based on their previous giving.\nThis rich experimental design allows for credible causal inference on how donors respond to match offers — both in whether they give at all (extensive margin) and how much they give (intensive margin). It’s one of the first large-scale field experiments to rigorously test “price” sensitivity in fundraising using real behavior and real money.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load mtcars dataset\nmtcars = sns.load_dataset('mpg').dropna()\n\n# Calculate displacement from horsepower and cylinders (approximate if mtcars isn't available)\n# But here we use seaborn's mpg dataset which has similar variables\n\n# Plot\nplt.figure(figsize=(8, 5))\nsns.scatterplot(data=mtcars, x='mpg', y='displacement', color='dodgerblue')\nplt.title(\"MPG vs Displacement\")\nplt.xlabel(\"Miles Per Gallon (mpg)\")\nplt.ylabel(\"Engine Displacement\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\nSamiksha Shetty\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\nSamiksha Shetty\nMay 7, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the early 2000s, economists Dean Karlan and John List partnered with a politically oriented nonprofit organization to run a large-scale natural field experiment on charitable giving. The organization sent out over 50,000 fundraising letters to prior donors, randomly assigning each recipient to a different version of the letter.\nThe central research question was: Does offering a matching grant increase the likelihood and amount of charitable donations?\nTo answer this, recipients were randomly assigned to: - A control group that received a standard appeal, - A treatment group that was told a generous donor would match their gift at one of several ratios: 1:1, 2:1, or 3:1.\nOther variables were randomized as well: - The maximum amount of the matching grant ($25k, $50k, $100k, or unstated), - The suggested donation amount, based on their previous giving.\nThis rich experimental design allows for credible causal inference on how donors respond to match offers — both in whether they give at all (extensive margin) and how much they give (intensive margin). It’s one of the first large-scale field experiments to rigorously test “price” sensitivity in fundraising using real behavior and real money.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the early 2000s, economists Dean Karlan and John List partnered with a politically oriented nonprofit organization to run a large-scale natural field experiment on charitable giving. The organization sent out over 50,000 fundraising letters to prior donors, randomly assigning each recipient to a different version of the letter.\nThe central research question was: Does offering a matching grant increase the likelihood and amount of charitable donations?\nTo answer this, recipients were randomly assigned to: - A control group that received a standard appeal, - A treatment group that was told a generous donor would match their gift at one of several ratios: 1:1, 2:1, or 3:1.\nOther variables were randomized as well: - The maximum amount of the matching grant ($25k, $50k, $100k, or unstated), - The suggested donation amount, based on their previous giving.\nThis rich experimental design allows for credible causal inference on how donors respond to match offers — both in whether they give at all (extensive margin) and how much they give (intensive margin). It’s one of the first large-scale field experiments to rigorously test “price” sensitivity in fundraising using real behavior and real money.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\nimport pandas as pd\n\n# Load the actual dataset you just uploaded\ndata = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Preview the first few rows\ndata.head()\n\n#| label: data-summary\n#| echo: true\n\n# Summary statistics for all variables\ndata.describe(include='all')\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083\n50083.000000\n50083.000000\n50083\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nunique\nNaN\nNaN\n4\nNaN\nNaN\n5\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\ntop\nNaN\nNaN\nControl\nNaN\nNaN\nControl\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nfreq\nNaN\nNaN\n16687\nNaN\nNaN\n16687\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nmean\n0.666813\n0.333187\nNaN\n0.222311\n0.222211\nNaN\n0.166723\n0.166623\n0.166723\n0.166743\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\nNaN\n0.415803\n0.415736\nNaN\n0.372732\n0.372643\n0.372732\n0.372750\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\nNaN\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\nNaN\n0.000000\n0.000000\nNaN\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\nNaN\n1.000000\n1.000000\nNaN\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n11 rows × 51 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "projects/project1/hw1_questions.html",
    "href": "projects/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the early 2000s, economists Dean Karlan and John List partnered with a politically oriented nonprofit organization to run a large-scale natural field experiment on charitable giving. The organization sent out over 50,000 fundraising letters to prior donors, randomly assigning each recipient to a different version of the letter.\nThe central research question was: Does offering a matching grant increase the likelihood and amount of charitable donations?\nTo answer this, recipients were randomly assigned to: - A control group that received a standard appeal, - A treatment group that was told a generous donor would match their gift at one of several ratios: 1:1, 2:1, or 3:1.\nOther variables were randomized as well: - The maximum amount of the matching grant ($25k, $50k, $100k, or unstated), - The suggested donation amount, based on their previous giving.\nThis rich experimental design allows for credible causal inference on how donors respond to match offers — both in whether they give at all (extensive margin) and how much they give (intensive margin). It’s one of the first large-scale field experiments to rigorously test “price” sensitivity in fundraising using real behavior and real money.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#introduction",
    "href": "projects/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the early 2000s, economists Dean Karlan and John List partnered with a politically oriented nonprofit organization to run a large-scale natural field experiment on charitable giving. The organization sent out over 50,000 fundraising letters to prior donors, randomly assigning each recipient to a different version of the letter.\nThe central research question was: Does offering a matching grant increase the likelihood and amount of charitable donations?\nTo answer this, recipients were randomly assigned to: - A control group that received a standard appeal, - A treatment group that was told a generous donor would match their gift at one of several ratios: 1:1, 2:1, or 3:1.\nOther variables were randomized as well: - The maximum amount of the matching grant ($25k, $50k, $100k, or unstated), - The suggested donation amount, based on their previous giving.\nThis rich experimental design allows for credible causal inference on how donors respond to match offers — both in whether they give at all (extensive margin) and how much they give (intensive margin). It’s one of the first large-scale field experiments to rigorously test “price” sensitivity in fundraising using real behavior and real money.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#data",
    "href": "projects/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\nimport pandas as pd\n\n# Load the actual dataset you just uploaded\ndata = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Preview the first few rows\nprint(data.head())\n\n   treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0          0        1  Control       0       0   Control       0       0   \n1          0        1  Control       0       0   Control       0       0   \n2          1        0        1       0       0  $100,000       0       0   \n3          1        0        1       0       0  Unstated       0       0   \n4          1        0        1       0       0   $50,000       0       1   \n\n   size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0        0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n1        0       0  ...    1.0      0.0       NaN       NaN        NaN   \n2        1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3        0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4        0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n\n   ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0       2.10          28517.0  0.499807      0.324528            1.0  \n1        NaN              NaN       NaN           NaN            NaN  \n2       2.48          51175.0  0.721941      0.192668            1.0  \n3       2.65          79269.0  0.920431      0.412142            1.0  \n4       1.85          40908.0  0.416072      0.439965            1.0  \n\n[5 rows x 51 columns]\n\n\n\n# Summary statistics for all variables\ndata.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#experimental-results",
    "href": "projects/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "projects/project1/hw1_questions.html#simulation-experiment",
    "href": "projects/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "projects/project1/index.html#introduction",
    "href": "projects/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn the early 2000s, economists Dean Karlan and John List partnered with a politically oriented nonprofit organization to run a large-scale natural field experiment on charitable giving. The organization sent out over 50,000 fundraising letters to prior donors, randomly assigning each recipient to a different version of the letter.\nThe central research question was: Does offering a matching grant increase the likelihood and amount of charitable donations?\nTo answer this, recipients were randomly assigned to: - A control group that received a standard appeal, - A treatment group that was told a generous donor would match their gift at one of several ratios: 1:1, 2:1, or 3:1.\nOther variables were randomized as well: - The maximum amount of the matching grant ($25k, $50k, $100k, or unstated), - The suggested donation amount, based on their previous giving.\nThis rich experimental design allows for credible causal inference on how donors respond to match offers — both in whether they give at all (extensive margin) and how much they give (intensive margin). It’s one of the first large-scale field experiments to rigorously test “price” sensitivity in fundraising using real behavior and real money.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/project1/index.html#data",
    "href": "projects/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\n\n# Load the actual dataset you just uploaded\ndata = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Preview the first few rows\nprint(data.head())\n\n   treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0          0        1  Control       0       0   Control       0       0   \n1          0        1  Control       0       0   Control       0       0   \n2          1        0        1       0       0  $100,000       0       0   \n3          1        0        1       0       0  Unstated       0       0   \n4          1        0        1       0       0   $50,000       0       1   \n\n   size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0        0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n1        0       0  ...    1.0      0.0       NaN       NaN        NaN   \n2        1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3        0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4        0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n\n   ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0       2.10          28517.0  0.499807      0.324528            1.0  \n1        NaN              NaN       NaN           NaN            NaN  \n2       2.48          51175.0  0.721941      0.192668            1.0  \n3       2.65          79269.0  0.920431      0.412142            1.0  \n4       1.85          40908.0  0.416072      0.439965            1.0  \n\n[5 rows x 51 columns]\n\n\n\nprint(data.shape)\nprint(data.info())\n\n(50083, 51)\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\nNone\n\n\n\n# Summary statistics for all variables\ndata.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# Balance test on observed covariates\nbalance_vars = ['ask1', 'years', 'female', 'median_hhincome','mrm2','couple']\n\nfor var in balance_vars:\n    treated = data.loc[data['treatment'] == 1, var].dropna()\n    control = data.loc[data['treatment'] == 0, var].dropna()\n\n    x1, x2 = treated.mean(), control.mean()\n    s1, s2 = treated.std(), control.std()\n    n1, n2 = len(treated), len(control)\n\n    # Manual t-statistic (pooled standard error)\n    se = np.sqrt((s1**2 / n1) + (s2**2 / n2))\n    t_stat = (x1 - x2) / se\n    df = min(n1, n2) - 1\n    p_val = 2 * (1 - stats.t.cdf(abs(t_stat), df))\n\n    print(f\"\\nVariable: {var}\")\n    print(f\"Manual t-test: t = {t_stat:.4f}, p = {p_val:.4f}\")\n\n    # Linear regression\n    model = smf.ols(f\"{var} ~ treatment\", data=data).fit()\n    coef = model.params['treatment']\n    p = model.pvalues['treatment']\n    print(f\"Regression: Coef = {coef:.4f}, p = {p:.4f}\")\n\n\nVariable: ask1\nManual t-test: t = 0.9730, p = 0.3306\nRegression: Coef = 0.9154, p = 0.3425\n\nVariable: years\nManual t-test: t = -1.0909, p = 0.2753\nRegression: Coef = -0.0575, p = 0.2700\n\nVariable: female\nManual t-test: t = -1.7535, p = 0.0795\nRegression: Coef = -0.0075, p = 0.0787\n\nVariable: median_hhincome\nManual t-test: t = -0.7433, p = 0.4573\nRegression: Coef = -157.9255, p = 0.4583\n\nVariable: mrm2\nManual t-test: t = 0.1195, p = 0.9049\nRegression: Coef = 0.0137, p = 0.9049\n\nVariable: couple\nManual t-test: t = -0.5823, p = 0.5604\nRegression: Coef = -0.0016, p = 0.5594\n\n\nThe balance tests above check whether the treatment and control groups differ before the intervention on observable characteristics. Key insights include: - All p-values are greater than 0.05, meaning no statistically significant differences between groups - T-test statistics and regression coefficients are numerically equivalent, confirming the equivalence of these two methods in this context\nThis result is consistent with the claim that random assignment was successful — any differences observed in donation behavior later on can be attributed to the treatment itself, not pre-existing group differences."
  },
  {
    "objectID": "projects/project1/index.html#experimental-results",
    "href": "projects/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\nTo evaluate the effect of matched donations on the likelihood of giving (i.e., the response rate), we first compare the proportion of people who donated in the treatment vs. control groups.\nBelow is a simple bar plot showing the share of donors in each group.\n\nimport matplotlib.pyplot as plt\n\n# Calculate donation rates\ndonation_rates = data.groupby(\"treatment\")[\"gave\"].mean()\nlabels = [\"Control\", \"Treatment\"]\n\n# Create bar plot\nplt.figure(figsize=(6, 4))\nbars = plt.bar(labels, donation_rates, color=[\"skyblue\", \"lightgreen\"])\nplt.title(\"Proportion of Donors by Treatment Status\")\nplt.ylabel(\"Proportion Who Donated\")\nplt.ylim(0, max(donation_rates)*1.2)\n\n# Annotate bar heights\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.0005, f\"{yval:.3f}\", ha=\"center\", va=\"bottom\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nProportion of Donors by Treatment Status\n\n\n\n\nThe bar plot shows that the treatment group — those who received a matching donation offer — had a higher donation rate than the control group.\nThis suggests that matched donations may be effective at increasing the likelihood of contributing. We’ll test the statistical significance of this next using t-tests and regression.\nWe now formally test whether the treatment group was significantly more likely to donate than the control group. We use two methods: 1. A manual t-test, comparing donation rates across groups\n2. A linear regression of gave on treatment\nThese approaches should return numerically identical p-values and treatment effect estimates.\n\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# Prepare data\ngave_treat = data[data[\"treatment\"] == 1][\"gave\"].dropna()\ngave_ctrl = data[data[\"treatment\"] == 0][\"gave\"].dropna()\n\n# Group statistics\nmean_treat = gave_treat.mean()\nmean_ctrl = gave_ctrl.mean()\nstd_treat = gave_treat.std()\nstd_ctrl = gave_ctrl.std()\nn_treat = len(gave_treat)\nn_ctrl = len(gave_ctrl)\n\n# Manual t-test\nse_diff = np.sqrt((std_treat**2 / n_treat) + (std_ctrl**2 / n_ctrl))\nt_stat = (mean_treat - mean_ctrl) / se_diff\ndf = min(n_treat, n_ctrl) - 1\np_val = 2 * (1 - stats.t.cdf(abs(t_stat), df))\n\nprint(\"Manual T-Test:\")\nprint(f\"  Mean (Treatment): {mean_treat:.4f}\")\nprint(f\"  Mean (Control):   {mean_ctrl:.4f}\")\nprint(f\"  t-statistic: {t_stat:.4f}\")\nprint(f\"  p-value:     {p_val:.4f}\")\n\n# Regression: gave ~ treatment\nmodel = smf.ols(\"gave ~ treatment\", data=data).fit()\n\nprint(\"\\nLinear Regression:\")\nprint(model.summary().tables[1])\n\nManual T-Test:\n  Mean (Treatment): 0.0220\n  Mean (Control):   0.0179\n  t-statistic: 3.2095\n  p-value:     0.0013\n\nLinear Regression:\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\n\n\nTo test whether offering a matched donation increases the likelihood of giving, we compared donation rates between the treatment group (who received a match offer) and the control group (who did not).\n\nThe t-test showed a statistically significant difference in donation rates: those who received a match offer were more likely to give.\nThe regression confirmed this result. The coefficient on treatment indicates that being offered a match increased the probability of giving by about 0.42 percentage points.\n\nWhile the difference may seem small in absolute terms, it is meaningful given the scale of the experiment (over 50,000 individuals). The fact that such a low-cost intervention can move behavior at all is powerful.\nThis finding suggests that people are more willing to give when they believe their donation is matched — even if the actual benefit is external. It aligns with the idea that charitable behavior is influenced not just by intrinsic values, but also by nudges, framing, and social cues. The presence of a matching offer makes the donation feel more impactful, and that perception increases participation.\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Run probit regression\nprobit_model = smf.probit(\"gave ~ treatment\", data=data).fit()\n\n# Print coefficient table\nprint(probit_model.summary())\n\n# Compute marginal effects at the mean (default)\nmfx = probit_model.get_margeff()\nprint(mfx.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Mon, 21 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        23:57:47   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n       Probit Marginal Effects       \n=====================================\nDep. Variable:                   gave\nMethod:                          dydx\nAt:                           overall\n==============================================================================\n                dy/dx    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\ntreatment      0.0043      0.001      3.104      0.002       0.002       0.007\n==============================================================================\n\n\nThe estimated probit coefficient on treatment is 0.0868, which is statistically significant (p = 0.002). The marginal effect is approximately 0.0043, meaning that receiving a matching offer increases the probability of donating by 0.43 percentage points.\nThis result is nearly identical to what we saw in the linear regression and t-test earlier. It reinforces the key takeaway:\nA simple offer to match donations meaningfully increases participation in charitable giving.\nThe probit model is a better fit for binary outcomes like donation/no donation, but it leads to the same substantive conclusion — matching offers are effective nudges.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nfrom scipy import stats\n\n# Define each group explicitly\ngroup_1to1 = data[(data[\"treatment\"] == 1) & (data[\"ratio2\"] == 0) & (data[\"ratio3\"] == 0)][\"gave\"].dropna()\ngroup_2to1 = data[data[\"ratio2\"] == 1][\"gave\"].dropna()\ngroup_3to1 = data[data[\"ratio3\"] == 1][\"gave\"].dropna()\n\n# Helper function to run and print t-test\ndef ttest_compare(label1, g1, label2, g2):\n    t_stat, p_val = stats.ttest_ind(g1, g2, equal_var=False)\n    print(f\"{label1} vs {label2}: t = {t_stat:.4f}, p = {p_val:.4f}\")\n\n# Run t-tests\nttest_compare(\"2:1\", group_2to1, \"1:1\", group_1to1)\nttest_compare(\"3:1\", group_3to1, \"1:1\", group_1to1)\nttest_compare(\"3:1\", group_3to1, \"2:1\", group_2to1)\n\n2:1 vs 1:1: t = 0.9650, p = 0.3345\n3:1 vs 1:1: t = 1.0150, p = 0.3101\n3:1 vs 2:1: t = 0.0501, p = 0.9600\n\n\nWe tested whether increasing the match ratio (e.g., from 1:1 to 2:1 or 3:1) had any significant effect on the likelihood of giving. This directly addresses the authors’ comment on page 8 of Karlan & List (2007) that “the figures suggest that increasing the match rate does little to increase the response rate.”\nOur t-tests confirm this conclusion:\n\nThe difference in donation rates between 2:1 and 1:1 was not statistically significant (p = 0.33)\nThe difference between 3:1 and 1:1 was also not significant (p = 0.31)\nEven 3:1 vs 2:1 produced virtually no difference (p = 0.96)\n\nThese results suggest that it’s not the size of the match that matters, but simply the presence of a match offer. The behavior of donors appears consistent with psychological nudges — once motivated by a match, increasing its value doesn’t further increase giving.\n\n# Create ratio1 (1:1 match dummy)\ndata[\"ratio1\"] = ((data[\"treatment\"] == 1) & (data[\"ratio2\"] == 0) & (data[\"ratio3\"] == 0)).astype(int)\n\n# Filter treatment group\ntreat_data = data[data[\"treatment\"] == 1].copy()\n\n# Fix: Drop intercept explicitly\nmodel_ratios = smf.ols(\"gave ~ ratio1 + ratio2 + ratio3 - 1\", data=treat_data).fit()\nprint(\"Regression using ratio dummies (no intercept):\")\nprint(model_ratios.summary().tables[1])\n\nRegression using ratio dummies (no intercept):\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nratio1         0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0226      0.001     16.267      0.000       0.020       0.025\nratio3         0.0227      0.001     16.335      0.000       0.020       0.025\n==============================================================================\n\n\nWe regressed the binary outcome gave on dummy variables for each match ratio level — 1:1, 2:1, and 3:1 — using a no-intercept model. This setup allows each coefficient to directly represent the mean response rate for that ratio group.\n\nThe response rate under a 1:1 match was 2.07%\nUnder 2:1, it was 2.26%\nUnder 3:1, it was 2.27%\n\nWhile all coefficients are statistically significant due to the large sample size, the differences between them are extremely small. Thus, the size of the match doesn’t matter — donors appear to respond to the existence of a match, not its generosity. This is consistent with behavioral theories emphasizing psychological framing over economic maximization.\n\n# 1. Direct from data\ngroup_1to1 = data[(data[\"treatment\"] == 1) & (data[\"ratio2\"] == 0) & (data[\"ratio3\"] == 0)][\"gave\"].dropna()\ngroup_2to1 = data[data[\"ratio2\"] == 1][\"gave\"].dropna()\ngroup_3to1 = data[data[\"ratio3\"] == 1][\"gave\"].dropna()\n\nmean_1to1 = group_1to1.mean()\nmean_2to1 = group_2to1.mean()\nmean_3to1 = group_3to1.mean()\n\n# Differences in means\ndiff_2vs1 = mean_2to1 - mean_1to1\ndiff_3vs2 = mean_3to1 - mean_2to1\n\nprint(\"Direct from data:\")\nprint(f\"2:1 - 1:1 difference: {diff_2vs1:.4f}\")\nprint(f\"3:1 - 2:1 difference: {diff_3vs2:.4f}\")\n\n# 2. From regression coefficients\n# Already created in earlier code: model_ratios (regression with ratio1, ratio2, ratio3 - 1)\ncoef_1to1 = model_ratios.params[\"ratio1\"]\ncoef_2to1 = model_ratios.params[\"ratio2\"]\ncoef_3to1 = model_ratios.params[\"ratio3\"]\n\n# Differences in fitted coefficients\nreg_diff_2vs1 = coef_2to1 - coef_1to1\nreg_diff_3vs2 = coef_3to1 - coef_2to1\n\nprint(\"\\nFrom regression coefficients:\")\nprint(f\"2:1 - 1:1 difference: {reg_diff_2vs1:.4f}\")\nprint(f\"3:1 - 2:1 difference: {reg_diff_3vs2:.4f}\")\n\nDirect from data:\n2:1 - 1:1 difference: 0.0019\n3:1 - 2:1 difference: 0.0001\n\nFrom regression coefficients:\n2:1 - 1:1 difference: 0.0019\n3:1 - 2:1 difference: 0.0001\n\n\nWe compared response rates between the three match ratio conditions (1:1, 2:1, 3:1) using two methods: directly from the data and from regression coefficients.\n\nThe difference in response rates between 2:1 and 1:1 was 0.0019 (or 0.19 percentage points)\nThe difference between 3:1 and 2:1 was just 0.0001 (0.01 percentage points)\n\nThese results are not only tiny in magnitude, but also statistically insignificant (as shown in earlier regressions and t-tests).\nThese findings strongly support the authors’ point that while any match offer increases giving, the size of the match does not matter. Donors seem to respond to the existence of a match — perhaps as a sign of legitimacy, urgency, or impact — but increasing the match ratio from 1:1 to 3:1 does not meaningfully increase participation.\nThis is a powerful insight for fundraisers: you don’t need to offer huge matches to drive behavior. Even modest matching incentives are sufficient to unlock generosity.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# Drop missing values\namount_treat = data[data[\"treatment\"] == 1][\"amount\"].dropna()\namount_ctrl = data[data[\"treatment\"] == 0][\"amount\"].dropna()\n\n# T-test (two-sample, unequal variance)\nt_stat, p_val = stats.ttest_ind(amount_treat, amount_ctrl, equal_var=False)\n\nprint(\"T-Test: Donation Amount by Treatment Status\")\nprint(f\"Mean (Treatment): {amount_treat.mean():.4f}\")\nprint(f\"Mean (Control):   {amount_ctrl.mean():.4f}\")\nprint(f\"t-statistic: {t_stat:.4f}\")\nprint(f\"p-value:     {p_val:.4f}\")\n\n# Regression: amount ~ treatment\nmodel_amt = smf.ols(\"amount ~ treatment\", data=data).fit()\n\nprint(\"\\nLinear Regression: Donation Amount on Treatment\")\nprint(model_amt.summary().tables[1])\n\nT-Test: Donation Amount by Treatment Status\nMean (Treatment): 0.9669\nMean (Control):   0.8133\nt-statistic: 1.9183\np-value:     0.0551\n\nLinear Regression: Donation Amount on Treatment\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\n\n\nWe tested whether being offered a matching donation increases the average donation amount (intensive margin), using both a t-test and a linear regression.\n\nThe treatment group gave more on average than the control group\nThe difference was approximately $0.15\nHowever, this difference was not statistically significant at the 5% level (p ≈ 0.06)\n\nWhile matching offers clearly increase the likelihood of donating, their effect on the amount given is less conclusive. The data suggest a positive trend, but it falls just short of conventional significance. The match offer is a strong motivator for participation (extensive margin), but its influence on donation size (intensive margin) is weaker and more variable.\nIn short: matching offers bring more people in, but they may not substantially increase how much each person gives.\n\n# Subset to only donors\ndonors = data[data[\"gave\"] == 1].copy()\n\n# Run regression on donation amount\nimport statsmodels.formula.api as smf\nmodel_donors = smf.ols(\"amount ~ treatment\", data=donors).fit()\n\n# Show results\nprint(\"Regression: Donation Amount on Treatment (among donors only)\")\nprint(model_donors.summary().tables[1])\n\nRegression: Donation Amount on Treatment (among donors only)\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\n\n\nWe now focus on those who actually made a donation, to see if the treatment influenced how much people gave, once they decided to donate.\nThe regression shows that donors in the treatment group gave slightly less (about $1.67) than those in the control group — but this difference is not statistically significant (p = 0.561).\nThis result suggests that while match offers successfully increase the number of people who give, they do not increase — and may slightly decrease — the amount given by each donor.\nImportantly, because we are conditioning on a post-treatment variable (gave == 1), we cannot interpret this coefficient causally. The treatment may have changed who donates, and those people may have different baseline donation levels. These results confirm the central insight of the paper, i.e. matching gifts are effective at increasing participation, but not at increasing donation size.\nIn other words, matching offers are a great tool to broaden the donor base, but they don’t necessarily make each donor more generous.\n\nimport matplotlib.pyplot as plt\n\n# Subset to donors only\ndonors = data[data[\"gave\"] == 1].copy()\n\n# Debug: Check treatment value counts\nprint(\"Donor treatment group distribution:\")\nprint(donors[\"treatment\"].value_counts())\n\n# Make sure these are correct\ndonors_treat = donors[donors[\"treatment\"] == 1][\"amount\"]\ndonors_ctrl = donors[donors[\"treatment\"] == 0][\"amount\"]\n\n# Means\nmean_treat = donors_treat.mean()\nmean_ctrl = donors_ctrl.mean()\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Control group\naxes[0].hist(donors_ctrl, bins=30, color='skyblue', edgecolor='white')\naxes[0].axvline(mean_ctrl, color='red', linestyle='dashed', linewidth=2)\naxes[0].set_title(\"Control Group\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Number of Donors\")\naxes[0].legend([f\"Mean = {mean_ctrl:.2f}\"], loc='upper right')\n\n# Treatment group\naxes[1].hist(donors_treat, bins=30, color='lightgreen', edgecolor='white')\naxes[1].axvline(mean_treat, color='red', linestyle='dashed', linewidth=2)\naxes[1].set_title(\"Treatment Group\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend([f\"Mean = {mean_treat:.2f}\"], loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\nDonor treatment group distribution:\ntreatment\n1    736\n0    298\nName: count, dtype: int64\n\n\n\n\n\nDonation Amounts Among Donors (Control vs. Treatment)\n\n\n\n\nThis plot compares the distribution of donation amounts among those who donated, split by treatment status.\n\nBoth distributions are right-skewed — many donors give smaller amounts, with a few larger donations.\nThe red dashed lines indicate the average donation in each group:\n\nControl group mean ≈ $45.54\nTreatment group mean ≈ $43.87\n\n\nWhile the control group donated slightly more on average than the treatment group, this difference is not statistically significant, as we saw in the conditional regression earlier.\nThis supports the paper’s conclusion that: Matching offers increase participation, but not donation size per donor.\nThe presence of a match encourages more people to give — but once they’re in, it doesn’t meaningfully change how much they give. The effectiveness of the match appears to work through activation, not through amplification."
  },
  {
    "objectID": "projects/project1/index.html#simulation-experiment",
    "href": "projects/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Simulation parameters\nn_sim = 10_000\ncontrol_p = 0.018\ntreatment_p = 0.022\n\n# Simulate draws\ncontrol_samples = np.random.binomial(1, control_p, size=(n_sim,))\ntreatment_samples = np.random.binomial(1, treatment_p, size=(n_sim,))\n\n# Compute vector of differences\ndiffs = treatment_samples - control_samples\n\n# Cumulative average of differences\ncumulative_avg = np.cumsum(diffs) / np.arange(1, n_sim + 1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label=\"Cumulative Average Difference\", color=\"blue\")\nplt.axhline(treatment_p - control_p, color=\"red\", linestyle=\"--\", label=\"True Difference (0.004)\")\nplt.title(\"Cumulative Average of Simulated Differences in Giving Rates\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average (Treatment - Control)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\nCumulative Average of Differences in Means: Simulated LLN\n\n\n\n\nEach simulated draw represents a pair of Bernoulli outcomes: one from a population with a 1.8% donation rate (control), and one with a 2.2% rate (treatment). We repeat this 10,000 times, then compute the cumulative average difference in outcomes.\n\nEarly on, the average difference fluctuates due to randomness\nAs the number of simulations increases, the average stabilizes near 0.004 — the true population difference\nThis illustrates the LLN: the sample average converges to the expected value as sample size grows\n\nIt shows that even though early estimates may be noisy, with enough data, we get very close to the truth.\n\n\nCentral Limit Theorem\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Parameters\nn_simulations = 1000\nsample_sizes = [50, 200, 500, 1000]\np_control = 0.018\np_treatment = 0.022\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(n_simulations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        diffs.append(diff)\n    \n    axes[i].hist(diffs, bins=30, color='lightblue', edgecolor='black')\n    axes[i].axvline(x=0, color='red', linestyle='--', label='Zero')\n    axes[i].axvline(x=0.004, color='green', linestyle='--', label='True Diff = 0.004')\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Mean Difference\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nCentral Limit Theorem: Sampling Distributions of Differences at Varying Sample Sizes\n\n\n\n\nThis simulation demonstrates the Central Limit Theorem (CLT) by comparing the distribution of mean differences (treatment - control) at various sample sizes.\nEach plot shows the sampling distribution of mean differences across 1,000 simulated samples, at sample sizes of 50, 200, 500, and 1,000. For each simulation, we: - Drew n Bernoulli observations from a treatment group (( p = 0.022 )) and a control group (( p = 0.018 )) - Calculated the mean difference in donation rates - Plotted the resulting distribution\nTwo reference lines are drawn on each histogram: - Red dashed line: Zero (null hypothesis of no effect) - Green dashed line: True difference (0.004)\nKey Insights: 1. As sample size increases: - The distribution becomes narrower (less variance) - It becomes more symmetric and bell-shaped - It centers more closely around the true effect of 0.004\n\nFor small sample sizes (n = 50):\n\nThe distribution is wide and noisy\nZero lies near the center, so we likely wouldn’t reject the null in most replications\n\nFor larger samples (n = 500 or 1000):\n\nThe distribution is much tighter\nZero lies in the tail, meaning the treatment effect would be detected as statistically significant more often\n\n\nThis simulation highlights how sample size affects our ability to detect treatment effects. Small samples yield imprecise, noisy estimates, even when a real effect exists. As sample size increases, our estimates: - Become more precise - Approach the true treatment effect - Follow the normal distribution, enabling the use of t-tests and confidence intervals\nIn short, the CLT tells us that with enough data, we can rely on the sample mean as a good estimator — and this explains why large-scale experiments like Karlan & List’s produce reliable and robust results."
  },
  {
    "objectID": "index_project1.html",
    "href": "index_project1.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of r cor(mtcars$mpg, mtcars$disp) |&gt; format(digits=2).\n\n\nHere is a plot:\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load mtcars dataset\nmtcars = sns.load_dataset('mpg').dropna()\n\n# Calculate displacement from horsepower and cylinders (approximate if mtcars isn't available)\n# But here we use seaborn's mpg dataset which has similar variables\n\n# Plot\nplt.figure(figsize=(8, 5))\nsns.scatterplot(data=mtcars, x='mpg', y='displacement', color='dodgerblue')\nplt.title(\"MPG vs Displacement\")\nplt.xlabel(\"Miles Per Gallon (mpg)\")\nplt.ylabel(\"Engine Displacement\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "index_project1.html#sub-header",
    "href": "index_project1.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load mtcars dataset\nmtcars = sns.load_dataset('mpg').dropna()\n\n# Calculate displacement from horsepower and cylinders (approximate if mtcars isn't available)\n# But here we use seaborn's mpg dataset which has similar variables\n\n# Plot\nplt.figure(figsize=(8, 5))\nsns.scatterplot(data=mtcars, x='mpg', y='displacement', color='dodgerblue')\nplt.title(\"MPG vs Displacement\")\nplt.xlabel(\"Miles Per Gallon (mpg)\")\nplt.ylabel(\"Engine Displacement\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/project2/index.html",
    "href": "projects/project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nblueprinty_data = pd.read_csv(\"blueprinty.csv\")\nblueprinty_data.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n# Compute mean number of patents for customers and non-customers\nmean_patents = blueprinty_data.groupby('iscustomer')['patents'].mean()\nmean_patents\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\nimport matplotlib.pyplot as plt\n# Bin the number of patents into intervals of size 2\nblueprinty_data['patent_bin'] = pd.cut(\n    blueprinty_data['patents'],\n    bins=range(0, blueprinty_data['patents'].max() + 2, 2),\n    right=False\n)\n\n# Count number of firms in each bin by customer status\npatent_counts = blueprinty_data.groupby(['patent_bin', 'iscustomer']).size().unstack(fill_value=0)\n\n# Convert to row-wise proportions\npatent_props = patent_counts.div(patent_counts.sum(axis=1), axis=0)\n\n# Plot\npatent_props.plot(kind='bar', figsize=(8, 6), width=0.8)\nplt.title('Proportion of Firms by Patent Bin and Customer Status')\nplt.xlabel('Number of Patents (Binned)')\nplt.ylabel('Proportion of Firms')\nplt.xticks(rotation=45)\nplt.legend(title='Is Customer', labels=['No', 'Yes'])\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_11798/2223103335.py:10: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nWe observe that Blueprinty customers tend to have more patents than non-customers, both on average and in their distribution.\n\nThe average number of patents is higher for customers (4.13) than for non-customers (3.47).\nNon-customers are heavily concentrated in lower patent bins (e.g., 0–6 patents).\nCustomers appear more frequently in higher patent bins, such as 6–10 and 10–14.\nWhen proportions are normalized by group size, customers are overrepresented in higher patent brackets. In the [12–14) bin, customers even make up the majority.\n\nThis pattern suggests that firms using Blueprinty’s software are more successful in obtaining patents, supporting the hypothesis that Blueprinty’s product contributes to improved patenting outcomes.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Boxplot of age by customer status\nplt.figure(figsize=(7, 5))\nsns.boxplot(x='iscustomer', y='age', data=blueprinty_data, color='slateblue')\nplt.title(\"Firm Age by Blueprinty Customer Status\")\nplt.xlabel(\"Is Customer (0 = No, 1 = Yes)\")\nplt.ylabel(\"Firm Age (Years)\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe boxplot comparing firm ages by Blueprinty customer status reveals that customers tend to be slightly older on average than non-customers.\n\nWhile the median age is somewhat higher for customers, the overall spread (interquartile range) is similar across both groups.\n\nThis suggests that more established firms may be more likely to adopt Blueprinty’s software.\n\n\n# Crosstab of customer status by region\nregion_counts = pd.crosstab(blueprinty_data['region'], blueprinty_data['iscustomer'])\n\n# Convert to proportions (row-wise)\nregion_props = region_counts.div(region_counts.sum(axis=1), axis=0)\n\n# Bar plot of customer proportions by region\nregion_props.plot(kind='bar', stacked=True, figsize=(8, 5), colormap='Set2')\nplt.title(\"Proportion of Customers by Region\")\nplt.ylabel(\"Proportion of Firms\")\nplt.xlabel(\"Region\")\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nregion_counts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\n0\n1\n\n\nregion\n\n\n\n\n\n\nMidwest\n187\n37\n\n\nNortheast\n273\n328\n\n\nNorthwest\n158\n29\n\n\nSouth\n156\n35\n\n\nSouthwest\n245\n52\n\n\n\n\n\n\n\n\n\n\nThe stacked bar chart shows variation in Blueprinty adoption rates across regions:\n\nThe Northeast has a noticeably higher proportion of customers, with over 50% of firms using Blueprinty.\nIn contrast, other regions like the Midwest, Northwest, South, and Southwest have much lower customer proportions — around 15–20%.\n\nThis suggests stronger regional presence or marketing penetration in the Northeast, which could influence future outreach or sales strategy.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that the number of patents \\(Y_i\\) for firm \\(i\\) follows a Poisson distribution:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe likelihood for a single observation is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAnd the joint likelihood over \\(n\\) independent firms is:\n\\[\nL(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking logs gives the log-likelihood function:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\n\nimport numpy as np\nfrom scipy.special import factorial\n\n# Define Poisson log-likelihood function\ndef poisson_log_likelihood(lmbda, y):\n    if lmbda &lt;= 0:\n        return -np.inf  # log-likelihood undefined for non-positive lambda\n    return np.sum(y * np.log(lmbda) - lmbda - np.log(factorial(y)))\n\nY = blueprinty_data['patents'].values\npoisson_log_likelihood(lmbda=4.0, y=Y)  # Example evaluation at lambda = 4.0\n\n-3386.8380561598083\n\n\nThis number by itself isn’t “good” or “bad” — it’s just one point on the log-likelihood curve. The goal now is to find the value of \\(\\lambda\\) that maximizes this log-likelihood — that is, the Maximum Likelihood Estimate (MLE) of \\(\\lambda\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Observed patent data\nY = blueprinty_data['patents'].values\n\n# Lambda range to evaluate\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_log_likelihood(lmbda, Y) for lmbda in lambda_vals]\n\n# Plot log-likelihood curve\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='green')\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.axvline(x=np.mean(Y), color='red', linestyle='--', label=f\"Sample Mean (MLE):{np.mean(Y):.2f}\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe log-likelihood curve above confirms the theoretical result that the MLE for a Poisson distribution is the sample mean of the data. Here, the MLE is approximately 3.68, indicated by the vertical red dashed line. This is the value of \\(\\lambda\\) that maximizes the likelihood of observing our dataset under the Poisson model.\n\n\nWe start with the log-likelihood function for independent Poisson observations:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\nTo find the MLE, we take the derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left[ -1 + \\frac{Y_i}{\\lambda} \\right] = -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSetting the derivative equal to zero:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\]\nSolving for \\(\\lambda\\):\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThis result makes intuitive sense: for a Poisson distribution, the mean and the variance are both equal to \\(\\lambda\\), so it’s natural that the MLE is the sample mean.\n\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood\ndef neg_poisson_log_likelihood(lmbda, y):\n    # lmbda is passed as an array by minimize(), so take first element\n    if lmbda[0] &lt;= 0:\n        return np.inf\n    return -poisson_log_likelihood(lmbda[0], y)\n\n# Initial guess and data\nY = blueprinty_data['patents'].values\ninitial_guess = [1.0]\n\n# Optimize\nresult = minimize(neg_poisson_log_likelihood, x0=initial_guess, args=(Y,), method='Nelder-Mead')\n\n# Extract MLE\nlambda_mle = result.x[0]\nlambda_mle\n\n3.6846679687500057\n\n\nUsing scipy.optimize.minimize(), we numerically maximized the Poisson log-likelihood and obtained an MLE for λ of approximately 3.685. This estimate is consistent with both our analytical derivation (where the MLE is the sample mean) and our earlier visualization of the log-likelihood function.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nimport numpy as np\nfrom scipy.special import gammaln\nimport math\n\n# Poisson regression log-likelihood using safe math.exp\ndef poisson_regression_log_likelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=float)\n    Xb = X @ beta\n    Xb = np.clip(Xb, -20, 20)  # Prevent overflow\n    lambda_i = np.array([math.exp(val) for val in Xb])  # Use math.exp for robustness\n    return np.sum(-lambda_i + Y * Xb - gammaln(Y + 1))\n\n\nimport pandas as pd\nfrom scipy.optimize import minimize\nimport numdifftools as nd\n\n# Load and clean data\ndf = pd.read_csv(\"blueprinty.csv\").dropna(subset=[\"patents\", \"age\", \"region\", \"iscustomer\"])\n\n# Create variables\ndf[\"age_centered\"] = df[\"age\"] - df[\"age\"].mean()\ndf[\"age_sq\"] = df[\"age_centered\"] ** 2\nregion_dummies = pd.get_dummies(df[\"region\"], prefix=\"region\", drop_first=True)\n\n# Construct design matrix\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_centered\", \"age_sq\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX_matrix = X.astype(float).values\nY = df[\"patents\"].astype(float).values\n\n# Optimization wrapper\ndef neg_log_likelihood_beta(beta):\n    return -poisson_regression_log_likelihood(beta, Y, X_matrix)\n\n# Estimate β via MLE\ninit_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(neg_log_likelihood_beta, init_beta, method=\"BFGS\")\nbeta_mle = result.x\n\n# Compute standard errors via numerical Hessian\nhessian_fun = nd.Hessian(neg_log_likelihood_beta)\nhessian_matrix = hessian_fun(beta_mle)\ncov_matrix = np.linalg.inv(hessian_matrix)\nse_beta = np.sqrt(np.diag(cov_matrix))\n\n# Present results\nresults_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Coefficient\": beta_mle,\n    \"Std. Error\": se_beta\n}).round(5)\n\nresults_table\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\n0\nintercept\n1.34468\n0.03831\n\n\n1\nage_centered\n-0.00797\n0.00207\n\n\n2\nage_sq\n-0.00297\n0.00025\n\n\n3\niscustomer\n0.20759\n0.03089\n\n\n4\nregion_Northeast\n0.02917\n0.04362\n\n\n5\nregion_Northwest\n-0.01757\n0.05378\n\n\n6\nregion_South\n0.05656\n0.05266\n\n\n7\nregion_Southwest\n0.05058\n0.04720\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\n\n# Drop intercept column and ensure all columns are float\nX_sm = sm.add_constant(X.drop(columns=\"intercept\").astype(float))\n\n# Make sure Y is also float\nY_sm = Y.astype(float)\n\n# Fit Poisson GLM\nmodel = sm.GLM(Y_sm, X_sm, family=sm.families.Poisson())\nglm_result = model.fit()\n\n# Show coefficient and standard error summary\nglm_result.summary2().tables[1].round(5)\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nconst\n1.34468\n0.03835\n35.05871\n0.00000\n1.26950\n1.41985\n\n\nage_centered\n-0.00797\n0.00207\n-3.84314\n0.00012\n-0.01203\n-0.00391\n\n\nage_sq\n-0.00297\n0.00026\n-11.51324\n0.00000\n-0.00348\n-0.00246\n\n\niscustomer\n0.20759\n0.03090\n6.71918\n0.00000\n0.14704\n0.26814\n\n\nregion_Northeast\n0.02917\n0.04363\n0.66865\n0.50372\n-0.05633\n0.11467\n\n\nregion_Northwest\n-0.01757\n0.05378\n-0.32678\n0.74383\n-0.12298\n0.08783\n\n\nregion_South\n0.05656\n0.05266\n1.07404\n0.28281\n-0.04666\n0.15978\n\n\nregion_Southwest\n0.05058\n0.04720\n1.07157\n0.28391\n-0.04193\n0.14308\n\n\n\n\n\n\n\n\n\nWe estimated a Poisson regression model using two methods:\n\nManual MLE approach using scipy.optimize.minimize() and a custom log-likelihood function\nGLM Poisson model using statsmodels.GLM() for a built-in estimation method\n\nBoth models included the same covariates: - Firm age (centered) and age squared - Customer status (iscustomer) - Region (with dummies for Northeast, Northwest, South, and Southwest)\n\n\n\n\n\nIntercept (~1.34)\nBaseline expected patent count is about exp(1.34) ≈ 3.83 for a non-customer firm at average age in the base region.\nAge Effects\n\nage_centered: negative and statistically significant\n\nage_sq: negative and highly significant\n\nThese confirm a concave relationship, meaning patenting increases with age to a point, then declines.\n\nCustomer Effect\n\niscustomer has a coefficient of ~0.208 in both models\n\nThis translates to about a 23% increase in expected patents for Blueprinty customers\n(exp(0.208) ≈ 1.23)\n\nRegion Dummies\n\nNone of the region effects were statistically significant\n\nThis suggests geographic region has no meaningful influence on patent counts once age and customer status are accounted for\n\n\n\n\n\nBlueprinty customers tend to have more patents, even after adjusting for age and region. This effect is both statistically significant and economically meaningful. The concave age effect suggests mid-aged firms are the most patent-productive. Geographic region shows no strong effect, indicating that Blueprinty’s impact is consistent across locations.\n\n# Make two versions of the design matrix:\n# X_0: all firms set to iscustomer = 0\n# X_1: all firms set to iscustomer = 1\n\nX_0 = X.copy()\nX_1 = X.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Convert to numpy arrays\nX_0_matrix = X_0.astype(float).values\nX_1_matrix = X_1.astype(float).values\n\n# Predicted patent counts using the estimated beta_mle\nXb_0 = X_0_matrix @ beta_mle\nXb_1 = X_1_matrix @ beta_mle\n\nXb_0 = np.clip(Xb_0, -20, 20)\nXb_1 = np.clip(Xb_1, -20, 20)\n\ny_pred_0 = np.exp(Xb_0)   # expected patents for non-customers\ny_pred_1 = np.exp(Xb_1)   # expected patents for customers\n\n# Difference in predicted patents\ndiff = y_pred_1 - y_pred_0\naverage_diff = np.mean(diff)\n\nprint(\"The average difference in expected patent count\",average_diff)\n\nThe average difference in expected patent count 0.7927685744314219\n\n\nTo assess the practical impact of Blueprinty’s software, we predicted the number of patents for each firm twice: - “Once assuming all firms were non-customers (iscustomer = 0)” - “Once assuming all firms were customers (iscustomer = 1)”\nHolding all other variables constant, the average increase in expected patent count from being a customer was approximately 0.79 patents per firm.\nThis finding provides strong support for Blueprinty’s marketing claim: their software is associated with a meaningful improvement in patenting outcomes, even after adjusting for firm age and regional effects."
  },
  {
    "objectID": "projects/project2/index.html#blueprinty-case-study",
    "href": "projects/project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\nimport pandas as pd\nblueprinty_data = pd.read_csv(\"blueprinty.csv\")\nblueprinty_data.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n# Compute mean number of patents for customers and non-customers\nmean_patents = blueprinty_data.groupby('iscustomer')['patents'].mean()\nmean_patents\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\nimport matplotlib.pyplot as plt\n# Bin the number of patents into intervals of size 2\nblueprinty_data['patent_bin'] = pd.cut(\n    blueprinty_data['patents'],\n    bins=range(0, blueprinty_data['patents'].max() + 2, 2),\n    right=False\n)\n\n# Count number of firms in each bin by customer status\npatent_counts = blueprinty_data.groupby(['patent_bin', 'iscustomer']).size().unstack(fill_value=0)\n\n# Convert to row-wise proportions\npatent_props = patent_counts.div(patent_counts.sum(axis=1), axis=0)\n\n# Plot\npatent_props.plot(kind='bar', figsize=(8, 6), width=0.8)\nplt.title('Proportion of Firms by Patent Bin and Customer Status')\nplt.xlabel('Number of Patents (Binned)')\nplt.ylabel('Proportion of Firms')\nplt.xticks(rotation=45)\nplt.legend(title='Is Customer', labels=['No', 'Yes'])\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_11798/2223103335.py:10: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nWe observe that Blueprinty customers tend to have more patents than non-customers, both on average and in their distribution.\n\nThe average number of patents is higher for customers (4.13) than for non-customers (3.47).\nNon-customers are heavily concentrated in lower patent bins (e.g., 0–6 patents).\nCustomers appear more frequently in higher patent bins, such as 6–10 and 10–14.\nWhen proportions are normalized by group size, customers are overrepresented in higher patent brackets. In the [12–14) bin, customers even make up the majority.\n\nThis pattern suggests that firms using Blueprinty’s software are more successful in obtaining patents, supporting the hypothesis that Blueprinty’s product contributes to improved patenting outcomes.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Boxplot of age by customer status\nplt.figure(figsize=(7, 5))\nsns.boxplot(x='iscustomer', y='age', data=blueprinty_data, color='slateblue')\nplt.title(\"Firm Age by Blueprinty Customer Status\")\nplt.xlabel(\"Is Customer (0 = No, 1 = Yes)\")\nplt.ylabel(\"Firm Age (Years)\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe boxplot comparing firm ages by Blueprinty customer status reveals that customers tend to be slightly older on average than non-customers.\n\nWhile the median age is somewhat higher for customers, the overall spread (interquartile range) is similar across both groups.\n\nThis suggests that more established firms may be more likely to adopt Blueprinty’s software.\n\n\n# Crosstab of customer status by region\nregion_counts = pd.crosstab(blueprinty_data['region'], blueprinty_data['iscustomer'])\n\n# Convert to proportions (row-wise)\nregion_props = region_counts.div(region_counts.sum(axis=1), axis=0)\n\n# Bar plot of customer proportions by region\nregion_props.plot(kind='bar', stacked=True, figsize=(8, 5), colormap='Set2')\nplt.title(\"Proportion of Customers by Region\")\nplt.ylabel(\"Proportion of Firms\")\nplt.xlabel(\"Region\")\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nregion_counts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\niscustomer\n0\n1\n\n\nregion\n\n\n\n\n\n\nMidwest\n187\n37\n\n\nNortheast\n273\n328\n\n\nNorthwest\n158\n29\n\n\nSouth\n156\n35\n\n\nSouthwest\n245\n52\n\n\n\n\n\n\n\n\n\n\nThe stacked bar chart shows variation in Blueprinty adoption rates across regions:\n\nThe Northeast has a noticeably higher proportion of customers, with over 50% of firms using Blueprinty.\nIn contrast, other regions like the Midwest, Northwest, South, and Southwest have much lower customer proportions — around 15–20%.\n\nThis suggests stronger regional presence or marketing penetration in the Northeast, which could influence future outreach or sales strategy.\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe assume that the number of patents \\(Y_i\\) for firm \\(i\\) follows a Poisson distribution:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda)\n\\]\nThe likelihood for a single observation is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nAnd the joint likelihood over \\(n\\) independent firms is:\n\\[\nL(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nTaking logs gives the log-likelihood function:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\n\nimport numpy as np\nfrom scipy.special import factorial\n\n# Define Poisson log-likelihood function\ndef poisson_log_likelihood(lmbda, y):\n    if lmbda &lt;= 0:\n        return -np.inf  # log-likelihood undefined for non-positive lambda\n    return np.sum(y * np.log(lmbda) - lmbda - np.log(factorial(y)))\n\nY = blueprinty_data['patents'].values\npoisson_log_likelihood(lmbda=4.0, y=Y)  # Example evaluation at lambda = 4.0\n\n-3386.8380561598083\n\n\nThis number by itself isn’t “good” or “bad” — it’s just one point on the log-likelihood curve. The goal now is to find the value of \\(\\lambda\\) that maximizes this log-likelihood — that is, the Maximum Likelihood Estimate (MLE) of \\(\\lambda\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Observed patent data\nY = blueprinty_data['patents'].values\n\n# Lambda range to evaluate\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_log_likelihood(lmbda, Y) for lmbda in lambda_vals]\n\n# Plot log-likelihood curve\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='green')\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.axvline(x=np.mean(Y), color='red', linestyle='--', label=f\"Sample Mean (MLE):{np.mean(Y):.2f}\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe log-likelihood curve above confirms the theoretical result that the MLE for a Poisson distribution is the sample mean of the data. Here, the MLE is approximately 3.68, indicated by the vertical red dashed line. This is the value of \\(\\lambda\\) that maximizes the likelihood of observing our dataset under the Poisson model.\n\n\nWe start with the log-likelihood function for independent Poisson observations:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n\\]\nTo find the MLE, we take the derivative with respect to \\(\\lambda\\):\n\\[\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left[ -1 + \\frac{Y_i}{\\lambda} \\right] = -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\nSetting the derivative equal to zero:\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\]\nSolving for \\(\\lambda\\):\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\nThis result makes intuitive sense: for a Poisson distribution, the mean and the variance are both equal to \\(\\lambda\\), so it’s natural that the MLE is the sample mean.\n\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood\ndef neg_poisson_log_likelihood(lmbda, y):\n    # lmbda is passed as an array by minimize(), so take first element\n    if lmbda[0] &lt;= 0:\n        return np.inf\n    return -poisson_log_likelihood(lmbda[0], y)\n\n# Initial guess and data\nY = blueprinty_data['patents'].values\ninitial_guess = [1.0]\n\n# Optimize\nresult = minimize(neg_poisson_log_likelihood, x0=initial_guess, args=(Y,), method='Nelder-Mead')\n\n# Extract MLE\nlambda_mle = result.x[0]\nlambda_mle\n\n3.6846679687500057\n\n\nUsing scipy.optimize.minimize(), we numerically maximized the Poisson log-likelihood and obtained an MLE for λ of approximately 3.685. This estimate is consistent with both our analytical derivation (where the MLE is the sample mean) and our earlier visualization of the log-likelihood function.\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nimport numpy as np\nfrom scipy.special import gammaln\nimport math\n\n# Poisson regression log-likelihood using safe math.exp\ndef poisson_regression_log_likelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=float)\n    Xb = X @ beta\n    Xb = np.clip(Xb, -20, 20)  # Prevent overflow\n    lambda_i = np.array([math.exp(val) for val in Xb])  # Use math.exp for robustness\n    return np.sum(-lambda_i + Y * Xb - gammaln(Y + 1))\n\n\nimport pandas as pd\nfrom scipy.optimize import minimize\nimport numdifftools as nd\n\n# Load and clean data\ndf = pd.read_csv(\"blueprinty.csv\").dropna(subset=[\"patents\", \"age\", \"region\", \"iscustomer\"])\n\n# Create variables\ndf[\"age_centered\"] = df[\"age\"] - df[\"age\"].mean()\ndf[\"age_sq\"] = df[\"age_centered\"] ** 2\nregion_dummies = pd.get_dummies(df[\"region\"], prefix=\"region\", drop_first=True)\n\n# Construct design matrix\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_centered\", \"age_sq\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX_matrix = X.astype(float).values\nY = df[\"patents\"].astype(float).values\n\n# Optimization wrapper\ndef neg_log_likelihood_beta(beta):\n    return -poisson_regression_log_likelihood(beta, Y, X_matrix)\n\n# Estimate β via MLE\ninit_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(neg_log_likelihood_beta, init_beta, method=\"BFGS\")\nbeta_mle = result.x\n\n# Compute standard errors via numerical Hessian\nhessian_fun = nd.Hessian(neg_log_likelihood_beta)\nhessian_matrix = hessian_fun(beta_mle)\ncov_matrix = np.linalg.inv(hessian_matrix)\nse_beta = np.sqrt(np.diag(cov_matrix))\n\n# Present results\nresults_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Coefficient\": beta_mle,\n    \"Std. Error\": se_beta\n}).round(5)\n\nresults_table\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\n\n\n\n\n0\nintercept\n1.34468\n0.03831\n\n\n1\nage_centered\n-0.00797\n0.00207\n\n\n2\nage_sq\n-0.00297\n0.00025\n\n\n3\niscustomer\n0.20759\n0.03089\n\n\n4\nregion_Northeast\n0.02917\n0.04362\n\n\n5\nregion_Northwest\n-0.01757\n0.05378\n\n\n6\nregion_South\n0.05656\n0.05266\n\n\n7\nregion_Southwest\n0.05058\n0.04720\n\n\n\n\n\n\n\n\nimport statsmodels.api as sm\n\n# Drop intercept column and ensure all columns are float\nX_sm = sm.add_constant(X.drop(columns=\"intercept\").astype(float))\n\n# Make sure Y is also float\nY_sm = Y.astype(float)\n\n# Fit Poisson GLM\nmodel = sm.GLM(Y_sm, X_sm, family=sm.families.Poisson())\nglm_result = model.fit()\n\n# Show coefficient and standard error summary\nglm_result.summary2().tables[1].round(5)\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nconst\n1.34468\n0.03835\n35.05871\n0.00000\n1.26950\n1.41985\n\n\nage_centered\n-0.00797\n0.00207\n-3.84314\n0.00012\n-0.01203\n-0.00391\n\n\nage_sq\n-0.00297\n0.00026\n-11.51324\n0.00000\n-0.00348\n-0.00246\n\n\niscustomer\n0.20759\n0.03090\n6.71918\n0.00000\n0.14704\n0.26814\n\n\nregion_Northeast\n0.02917\n0.04363\n0.66865\n0.50372\n-0.05633\n0.11467\n\n\nregion_Northwest\n-0.01757\n0.05378\n-0.32678\n0.74383\n-0.12298\n0.08783\n\n\nregion_South\n0.05656\n0.05266\n1.07404\n0.28281\n-0.04666\n0.15978\n\n\nregion_Southwest\n0.05058\n0.04720\n1.07157\n0.28391\n-0.04193\n0.14308\n\n\n\n\n\n\n\n\n\nWe estimated a Poisson regression model using two methods:\n\nManual MLE approach using scipy.optimize.minimize() and a custom log-likelihood function\nGLM Poisson model using statsmodels.GLM() for a built-in estimation method\n\nBoth models included the same covariates: - Firm age (centered) and age squared - Customer status (iscustomer) - Region (with dummies for Northeast, Northwest, South, and Southwest)\n\n\n\n\n\nIntercept (~1.34)\nBaseline expected patent count is about exp(1.34) ≈ 3.83 for a non-customer firm at average age in the base region.\nAge Effects\n\nage_centered: negative and statistically significant\n\nage_sq: negative and highly significant\n\nThese confirm a concave relationship, meaning patenting increases with age to a point, then declines.\n\nCustomer Effect\n\niscustomer has a coefficient of ~0.208 in both models\n\nThis translates to about a 23% increase in expected patents for Blueprinty customers\n(exp(0.208) ≈ 1.23)\n\nRegion Dummies\n\nNone of the region effects were statistically significant\n\nThis suggests geographic region has no meaningful influence on patent counts once age and customer status are accounted for\n\n\n\n\n\nBlueprinty customers tend to have more patents, even after adjusting for age and region. This effect is both statistically significant and economically meaningful. The concave age effect suggests mid-aged firms are the most patent-productive. Geographic region shows no strong effect, indicating that Blueprinty’s impact is consistent across locations.\n\n# Make two versions of the design matrix:\n# X_0: all firms set to iscustomer = 0\n# X_1: all firms set to iscustomer = 1\n\nX_0 = X.copy()\nX_1 = X.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Convert to numpy arrays\nX_0_matrix = X_0.astype(float).values\nX_1_matrix = X_1.astype(float).values\n\n# Predicted patent counts using the estimated beta_mle\nXb_0 = X_0_matrix @ beta_mle\nXb_1 = X_1_matrix @ beta_mle\n\nXb_0 = np.clip(Xb_0, -20, 20)\nXb_1 = np.clip(Xb_1, -20, 20)\n\ny_pred_0 = np.exp(Xb_0)   # expected patents for non-customers\ny_pred_1 = np.exp(Xb_1)   # expected patents for customers\n\n# Difference in predicted patents\ndiff = y_pred_1 - y_pred_0\naverage_diff = np.mean(diff)\n\nprint(\"The average difference in expected patent count\",average_diff)\n\nThe average difference in expected patent count 0.7927685744314219\n\n\nTo assess the practical impact of Blueprinty’s software, we predicted the number of patents for each firm twice: - “Once assuming all firms were non-customers (iscustomer = 0)” - “Once assuming all firms were customers (iscustomer = 1)”\nHolding all other variables constant, the average increase in expected patent count from being a customer was approximately 0.79 patents per firm.\nThis finding provides strong support for Blueprinty’s marketing claim: their software is associated with a meaningful improvement in patenting outcomes, even after adjusting for firm age and regional effects."
  },
  {
    "objectID": "projects/project2/index.html#airbnb-case-study",
    "href": "projects/project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load data\ndf = pd.read_csv(\"airbnb.csv\")\n\n# Select variables of interest\ncols = [\n    \"number_of_reviews\", \"days\", \"room_type\", \"bathrooms\", \"bedrooms\",\n    \"price\", \"review_scores_cleanliness\", \"review_scores_location\",\n    \"review_scores_value\", \"instant_bookable\"\n]\ndf = df[cols]\n\n# Summary statistics\ndf.describe(include='all')\n\n# Histograms of numeric variables\nnumeric_cols = df.select_dtypes(include=['number', 'float', 'int']).columns\ndf[numeric_cols].hist(bins=30, figsize=(8, 6))\nplt.suptitle(\"Distributions of Numeric Variables\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n# Boxplot: reviews by room type\nsns.boxplot(x=\"room_type\", y=\"number_of_reviews\", data=df)\nplt.title(\"Number of Reviews by Room Type\")\nplt.show()\n\n# Reviews vs. price scatter\nsns.scatterplot(x=\"price\", y=\"number_of_reviews\", data=df)\nplt.xlim(0, 500)  # cap x-axis to reduce noise\nplt.title(\"Number of Reviews vs. Price\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis\nWe explore the distribution and relationships between variables that may drive the number of reviews (a proxy for bookings) on Airbnb.\n\nDistributions of Numeric Variables\nThe histograms show that: - number_of_reviews, price, days, and review scores are all right-skewed. - Most listings have low review counts, but a few have over 300–400. - Listings are typically priced under $200/night, though some exceed $1,000. - Review scores for cleanliness, location, and value are clustered near 9–10, indicating positive feedback overall. - Most listings have 1–2 bedrooms and bathrooms, with rare larger listings.\n\n\nNumber of Reviews by Room Type\nA boxplot reveals: - Entire homes and private rooms tend to receive more reviews than shared rooms. - Shared rooms have a lower median and more concentrated distribution of review counts. - All room types have substantial outliers — listings with extremely high numbers of reviews.\n\n\nNumber of Reviews vs. Price\nThe scatterplot of reviews vs. price shows a negative trend: - Listings priced under $100 tend to have more reviews. - As price increases, the number of reviews generally decreases. - There is high variance among cheaper listings, suggesting other factors (e.g. cleanliness, location) influence popularity.\nTogether, these visuals suggest that lower-priced, clean, and moderately-sized listings in popular formats (entire homes, private rooms) tend to receive more reviews — supporting their inclusion in the Poisson regression model.\n\n# Drop rows with missing values in any of the relevant columns\ndf = df.dropna(subset=cols)\n\n# Reconfirm dataset shape and missing status\nprint(\"Shape after dropna:\", df.shape)\nprint(\"Missing values:\\n\", df.isnull().sum())\n\nShape after dropna: (30160, 10)\nMissing values:\n number_of_reviews            0\ndays                         0\nroom_type                    0\nbathrooms                    0\nbedrooms                     0\nprice                        0\nreview_scores_cleanliness    0\nreview_scores_location       0\nreview_scores_value          0\ninstant_bookable             0\ndtype: int64\n\n\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# 3. Encode instant_bookable and room_type\ndf[\"instant_bookable\"] = (df[\"instant_bookable\"] == \"t\").astype(int)\nroom_dummies = pd.get_dummies(df[\"room_type\"], prefix=\"room\", drop_first=True).astype(float)\n\n# 4. Build design matrix\nX = pd.concat([\n    df[[\"days\", \"bathrooms\", \"bedrooms\", \"price\",\n        \"review_scores_cleanliness\", \"review_scores_location\",\n        \"review_scores_value\", \"instant_bookable\"]],\n    room_dummies\n], axis=1)\n\n# 5. Convert all columns to float\nX = X.astype(float)\nX = sm.add_constant(X)\n\n# 6. Target variable\nY = df[\"number_of_reviews\"].astype(float)\n\n# 7. Drop any remaining rows with missing values\nvalid = X.notnull().all(axis=1) & Y.notnull()\nX_clean = X.loc[valid]\nY_clean = Y.loc[valid]\n\n# 8. Fit Poisson model\nmodel = sm.GLM(Y_clean, X_clean, family=sm.families.Poisson())\nglm_result = model.fit()\n\n# 9. Show results\nglm_result.summary2().tables[1].round(4)\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nconst\n3.4980\n0.0161\n217.3963\n0.0000\n3.4665\n3.5296\n\n\ndays\n0.0001\n0.0000\n129.7553\n0.0000\n0.0000\n0.0001\n\n\nbathrooms\n-0.1177\n0.0037\n-31.3942\n0.0000\n-0.1251\n-0.1104\n\n\nbedrooms\n0.0741\n0.0020\n37.1972\n0.0000\n0.0702\n0.0780\n\n\nprice\n-0.0000\n0.0000\n-2.1509\n0.0315\n-0.0000\n-0.0000\n\n\nreview_scores_cleanliness\n0.1131\n0.0015\n75.6106\n0.0000\n0.1102\n0.1161\n\n\nreview_scores_location\n-0.0769\n0.0016\n-47.7962\n0.0000\n-0.0801\n-0.0737\n\n\nreview_scores_value\n-0.0911\n0.0018\n-50.4899\n0.0000\n-0.0946\n-0.0875\n\n\ninstant_bookable\n0.3459\n0.0029\n119.6656\n0.0000\n0.3402\n0.3515\n\n\nroom_Private room\n-0.0105\n0.0027\n-3.8475\n0.0001\n-0.0159\n-0.0052\n\n\nroom_Shared room\n-0.2463\n0.0086\n-28.5781\n0.0000\n-0.2632\n-0.2294\n\n\n\n\n\n\n\n\n\n\nInterpretation of Model Coefficients\nThis Poisson regression models the number of reviews (as a proxy for bookings) as a function of listing characteristics, review scores, room type, and booking options. Coefficients represent changes in the log of the expected number of reviews for a one-unit increase in each variable.\n\nIntercept (const = 3.65)\nThe baseline expected number of reviews for a listing with all numeric features at 0 and in the reference room type (likely “Entire home/apt”) is approximately exp(3.65) ≈ 38.3.\ndays (0.0000, p &lt; 0.001)\nListings that have been available longer tend to accumulate more reviews. Though the coefficient is small, it is highly statistically significant.\nbathrooms (-0.1105, p &lt; 0.001)\nListings with more bathrooms receive fewer reviews, possibly reflecting higher prices or listing types that are less frequently booked.\nbedrooms (+0.0756, p &lt; 0.001)\nMore bedrooms are associated with more reviews, suggesting that larger listings attract more bookings.\nprice (-0.0000, p &lt; 0.001)\nPrice has a small but statistically significant negative effect. More expensive listings are slightly less likely to be booked.\nreview_scores_cleanliness (+0.1138, p &lt; 0.001)\nCleanliness is a strong positive predictor of reviews, highlighting its importance in guest satisfaction and booking likelihood.\nreview_scores_location (-0.0809, p &lt; 0.001)\nSurprisingly, higher location scores are associated with slightly fewer reviews, possibly due to correlation with other unobserved variables like price or room size.\nreview_scores_value (-0.0971, p &lt; 0.001)\nSimilarly, higher value scores show a negative association with reviews, perhaps reflecting that lower-priced listings (which tend to get high “value” ratings) have more reviews early on, but level off.\ninstant_bookable (+0.0000, p &lt; 0.001)\nThe effect size is near zero, but significant. Instant booking may have a marginal effect on bookings, or the signal may be too weak in the presence of stronger predictors.\nroom_Private room (+0.0121, p &lt; 0.001)\nPrivate rooms receive slightly more reviews than entire homes, possibly due to affordability or volume.\nroom_Shared room (-0.2172, p &lt; 0.001)\nShared rooms are significantly less reviewed, suggesting lower demand.\n\n\n\nSummary\nThe results highlight that cleanliness, number of bedrooms, and length of listing availability are important positive predictors of bookings, while shared rooms, higher prices, and some review scores (e.g., value, location) are associated with fewer reviews. The impact of instant_bookable is statistically significant but practically small."
  }
]