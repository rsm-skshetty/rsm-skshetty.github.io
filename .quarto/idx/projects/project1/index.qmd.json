{"title":"A Replication of Karlan and List (2007)","markdown":{"yaml":{"title":"A Replication of Karlan and List (2007)","author":"Samiksha Shetty","date":"today","callout-appearance":"minimal"},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n\nDean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the _American Economic Review_ in 2007. The article and supporting data are available from the [AEA website](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774) and from Innovations for Poverty Action as part of [Harvard's Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27853&version=4.2).\n\nIn the early 2000s, economists Dean Karlan and John List partnered with a politically oriented nonprofit organization to run a large-scale natural field experiment on charitable giving. The organization sent out over 50,000 fundraising letters to prior donors, randomly assigning each recipient to a different version of the letter.\n\nThe central research question was: Does offering a matching grant increase the likelihood and amount of charitable donations?\n\nTo answer this, recipients were randomly assigned to:\n- A control group that received a standard appeal,\n- A treatment group that was told a generous donor would match their gift at one of several ratios: 1:1, 2:1, or 3:1.\n\nOther variables were randomized as well:\n- The maximum amount of the matching grant (\\$25k, \\$50k, \\$100k, or unstated),\n- The suggested donation amount, based on their previous giving.\n\nThis rich experimental design allows for credible causal inference on how donors respond to match offers — both in whether they give at all (extensive margin) and how much they give (intensive margin). It’s one of the first large-scale field experiments to rigorously test “price” sensitivity in fundraising using real behavior and real money.\n\nThis project seeks to replicate their results.\n\n\n## Data\n\n### Description\n\n```{python}\n#| label: load-and-describe-data\n#| echo: true\n#| fig-cap-location: bottom\n\nimport pandas as pd\n\n# Load the actual dataset you just uploaded\ndata = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Preview the first few rows\nprint(data.head())\n```\n\n```{python}\nprint(data.shape)\nprint(data.info())\n```\n\n```{python}\n#| label: data-summary\n#| echo: true\n\n# Summary statistics for all variables\ndata.describe()\n\n```\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n| Variable             | Description                                                         |\n|----------------------|---------------------------------------------------------------------|\n| `treatment`          | Treatment                                                           |\n| `control`            | Control                                                             |\n| `ratio`              | Match ratio                                                         |\n| `ratio2`             | 2:1 match ratio                                                     |\n| `ratio3`             | 3:1 match ratio                                                     |\n| `size`               | Match threshold                                                     |\n| `size25`             | \\$25,000 match threshold                                            |\n| `size50`             | \\$50,000 match threshold                                            |\n| `size100`            | \\$100,000 match threshold                                           |\n| `sizeno`             | Unstated match threshold                                            |\n| `ask`                | Suggested donation amount                                           |\n| `askd1`              | Suggested donation was highest previous contribution                |\n| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |\n| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |\n| `ask1`               | Highest previous contribution (for suggestion)                      |\n| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |\n| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |\n| `amount`             | Dollars given                                                       |\n| `gave`               | Gave anything                                                       |\n| `amountchange`       | Change in amount given                                              |\n| `hpa`                | Highest previous contribution                                       |\n| `ltmedmra`           | Small prior donor: last gift was less than median \\$35              |\n| `freq`               | Number of prior donations                                           |\n| `years`              | Number of years since initial donation                              |\n| `year5`              | At least 5 years since initial donation                             |\n| `mrm2`               | Number of months since last donation                                |\n| `dormant`            | Already donated in 2005                                             |\n| `female`             | Female                                                              |\n| `couple`             | Couple                                                              |\n| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |\n| `nonlit`             | Nonlitigation                                                       |\n| `cases`              | Court cases from state in 2004-5 in which organization was involved |\n| `statecnt`           | Percent of sample from state                                        |\n| `stateresponse`      | Proportion of sample from the state who gave                        |\n| `stateresponset`     | Proportion of treated sample from the state who gave                |\n| `stateresponsec`     | Proportion of control sample from the state who gave                |\n| `stateresponsetminc` | stateresponset - stateresponsec                                     |\n| `perbush`            | State vote share for Bush                                           |\n| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |\n| `red0`               | Red state                                                           |\n| `blue0`              | Blue state                                                          |\n| `redcty`             | Red county                                                          |\n| `bluecty`            | Blue county                                                         |\n| `pwhite`             | Proportion white within zip code                                    |\n| `pblack`             | Proportion black within zip code                                    |\n| `page18_39`          | Proportion age 18-39 within zip code                                |\n| `ave_hh_sz`          | Average household size within zip code                              |\n| `median_hhincome`    | Median household income within zip code                             |\n| `powner`             | Proportion house owner within zip code                              |\n| `psch_atlstba`       | Proportion who finished college within zip code                     |\n| `pop_propurban`      | Proportion of population urban within zip code                      |\n\n::::\n\n\n### Balance Test \n\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n```{python}\n#| label: balance-ttests\n#| echo: true\n\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# Balance test on observed covariates\nbalance_vars = ['ask1', 'years', 'female', 'median_hhincome','mrm2','couple']\n\nfor var in balance_vars:\n    treated = data.loc[data['treatment'] == 1, var].dropna()\n    control = data.loc[data['treatment'] == 0, var].dropna()\n\n    x1, x2 = treated.mean(), control.mean()\n    s1, s2 = treated.std(), control.std()\n    n1, n2 = len(treated), len(control)\n\n    # Manual t-statistic (pooled standard error)\n    se = np.sqrt((s1**2 / n1) + (s2**2 / n2))\n    t_stat = (x1 - x2) / se\n    df = min(n1, n2) - 1\n    p_val = 2 * (1 - stats.t.cdf(abs(t_stat), df))\n\n    print(f\"\\nVariable: {var}\")\n    print(f\"Manual t-test: t = {t_stat:.4f}, p = {p_val:.4f}\")\n\n    # Linear regression\n    model = smf.ols(f\"{var} ~ treatment\", data=data).fit()\n    coef = model.params['treatment']\n    p = model.pvalues['treatment']\n    print(f\"Regression: Coef = {coef:.4f}, p = {p:.4f}\")\n\n```\n\nThe balance tests above check whether the treatment and control groups differ before the intervention on observable characteristics.\nKey insights include:\n- All p-values are greater than 0.05, meaning no statistically significant differences between groups\n- T-test statistics and regression coefficients are numerically equivalent, confirming the equivalence of these two methods in this context\n\nThis result is consistent with the claim that random assignment was successful — any differences observed in donation behavior later on can be attributed to the treatment itself, not pre-existing group differences.\n\n\n## Experimental Results\n\n### Charitable Contribution Made\n\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation. \n\nTo evaluate the effect of matched donations on the likelihood of giving (i.e., the response rate), we first compare the proportion of people who donated in the treatment vs. control groups.\n\nBelow is a simple bar plot showing the share of donors in each group.\n\n```{python}\n#| label: barplot-response-rate\n#| fig-cap: \"Proportion of Donors by Treatment Status\"\n#| fig-align: center\n\nimport matplotlib.pyplot as plt\n\n# Calculate donation rates\ndonation_rates = data.groupby(\"treatment\")[\"gave\"].mean()\nlabels = [\"Control\", \"Treatment\"]\n\n# Create bar plot\nplt.figure(figsize=(6, 4))\nbars = plt.bar(labels, donation_rates, color=[\"skyblue\", \"lightgreen\"])\nplt.title(\"Proportion of Donors by Treatment Status\")\nplt.ylabel(\"Proportion Who Donated\")\nplt.ylim(0, max(donation_rates)*1.2)\n\n# Annotate bar heights\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.0005, f\"{yval:.3f}\", ha=\"center\", va=\"bottom\")\n\nplt.tight_layout()\nplt.show()\n```\nThe bar plot shows that the treatment group — those who received a matching donation offer — had a higher donation rate than the control group.\n\nThis suggests that matched donations may be effective at increasing the likelihood of contributing. We'll test the statistical significance of this next using t-tests and regression.\n\n\nWe now formally test whether the treatment group was significantly more likely to donate than the control group. \nWe use two methods:\n1. A manual t-test, comparing donation rates across groups  \n2. A linear regression of gave on treatment\n\nThese approaches should return numerically identical p-values and treatment effect estimates.\n\n```{python}\n#| label: ttest-and-reg-gave\n#| echo: true\n\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# Prepare data\ngave_treat = data[data[\"treatment\"] == 1][\"gave\"].dropna()\ngave_ctrl = data[data[\"treatment\"] == 0][\"gave\"].dropna()\n\n# Group statistics\nmean_treat = gave_treat.mean()\nmean_ctrl = gave_ctrl.mean()\nstd_treat = gave_treat.std()\nstd_ctrl = gave_ctrl.std()\nn_treat = len(gave_treat)\nn_ctrl = len(gave_ctrl)\n\n# Manual t-test\nse_diff = np.sqrt((std_treat**2 / n_treat) + (std_ctrl**2 / n_ctrl))\nt_stat = (mean_treat - mean_ctrl) / se_diff\ndf = min(n_treat, n_ctrl) - 1\np_val = 2 * (1 - stats.t.cdf(abs(t_stat), df))\n\nprint(\"Manual T-Test:\")\nprint(f\"  Mean (Treatment): {mean_treat:.4f}\")\nprint(f\"  Mean (Control):   {mean_ctrl:.4f}\")\nprint(f\"  t-statistic: {t_stat:.4f}\")\nprint(f\"  p-value:     {p_val:.4f}\")\n\n# Regression: gave ~ treatment\nmodel = smf.ols(\"gave ~ treatment\", data=data).fit()\n\nprint(\"\\nLinear Regression:\")\nprint(model.summary().tables[1])\n```\nTo test whether offering a matched donation increases the likelihood of giving, we compared donation rates between the treatment group (who received a match offer) and the control group (who did not).\n\n- The t-test showed a statistically significant difference in donation rates: those who received a match offer were more likely to give.\n- The regression confirmed this result. The coefficient on `treatment` indicates that being offered a match increased the probability of giving by about 0.42 percentage points.\n\nWhile the difference may seem small in absolute terms, it is meaningful given the scale of the experiment (over 50,000 individuals). The fact that such a low-cost intervention can move behavior at all is powerful.\n\nThis finding suggests that people are more willing to give when they believe their donation is matched — even if the actual benefit is external. It aligns with the idea that charitable behavior is influenced not just by intrinsic values, but also by nudges, framing, and social cues. The presence of a matching offer makes the donation feel more impactful, and that perception increases participation.\n\n\n```{python}\n#| label: probit-gave-treatment\n#| echo: true\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Run probit regression\nprobit_model = smf.probit(\"gave ~ treatment\", data=data).fit()\n\n# Print coefficient table\nprint(probit_model.summary())\n\n# Compute marginal effects at the mean (default)\nmfx = probit_model.get_margeff()\nprint(mfx.summary())\n```\n\n\nThe estimated probit coefficient on `treatment` is 0.0868, which is statistically significant (p = 0.002). The marginal effect is approximately 0.0043, meaning that receiving a matching offer increases the probability of donating by 0.43 percentage points.\n\nThis result is nearly identical to what we saw in the linear regression and t-test earlier. It reinforces the key takeaway:  \n**A simple offer to match donations meaningfully increases participation in charitable giving.**\n\nThe probit model is a better fit for binary outcomes like donation/no donation, but it leads to the same substantive conclusion — matching offers are effective nudges.\n\n\n### Differences between Match Rates\n\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n```{python}\n#| label: ttests-match-ratios\n#| echo: true\n\nfrom scipy import stats\n\n# Define each group explicitly\ngroup_1to1 = data[(data[\"treatment\"] == 1) & (data[\"ratio2\"] == 0) & (data[\"ratio3\"] == 0)][\"gave\"].dropna()\ngroup_2to1 = data[data[\"ratio2\"] == 1][\"gave\"].dropna()\ngroup_3to1 = data[data[\"ratio3\"] == 1][\"gave\"].dropna()\n\n# Helper function to run and print t-test\ndef ttest_compare(label1, g1, label2, g2):\n    t_stat, p_val = stats.ttest_ind(g1, g2, equal_var=False)\n    print(f\"{label1} vs {label2}: t = {t_stat:.4f}, p = {p_val:.4f}\")\n\n# Run t-tests\nttest_compare(\"2:1\", group_2to1, \"1:1\", group_1to1)\nttest_compare(\"3:1\", group_3to1, \"1:1\", group_1to1)\nttest_compare(\"3:1\", group_3to1, \"2:1\", group_2to1)\n\n```\nWe tested whether increasing the match ratio (e.g., from 1:1 to 2:1 or 3:1) had any significant effect on the likelihood of giving. This directly addresses the authors' comment on page 8 of Karlan & List (2007) that \"*the figures suggest that increasing the match rate does little to increase the response rate.*\"\n\nOur t-tests confirm this conclusion:\n\n- The difference in donation rates between 2:1 and 1:1 was not statistically significant (p = 0.33)\n- The difference between 3:1 and 1:1 was also not significant (p = 0.31)\n- Even 3:1 vs 2:1 produced virtually no difference (p = 0.96)\n\nThese results suggest that it’s not the size of the match that matters, but simply the presence of a match offer. The behavior of donors appears consistent with psychological nudges — once motivated by a match, increasing its value doesn’t further increase giving.\n\n```{python}\n#| label: regression-ratio-fix\n#| echo: true\n\n# Create ratio1 (1:1 match dummy)\ndata[\"ratio1\"] = ((data[\"treatment\"] == 1) & (data[\"ratio2\"] == 0) & (data[\"ratio3\"] == 0)).astype(int)\n\n# Filter treatment group\ntreat_data = data[data[\"treatment\"] == 1].copy()\n\n# Fix: Drop intercept explicitly\nmodel_ratios = smf.ols(\"gave ~ ratio1 + ratio2 + ratio3 - 1\", data=treat_data).fit()\nprint(\"Regression using ratio dummies (no intercept):\")\nprint(model_ratios.summary().tables[1])\n```\n\nWe regressed the binary outcome `gave` on dummy variables for each match ratio level — 1:1, 2:1, and 3:1 — using a no-intercept model. This setup allows each coefficient to directly represent the mean response rate for that ratio group.\n\n- The response rate under a 1:1 match was 2.07%\n- Under 2:1, it was 2.26%\n- Under 3:1, it was 2.27%\n\nWhile all coefficients are statistically significant due to the large sample size, the differences between them are extremely small. Thus, the size of the match doesn’t matter — donors appear to respond to the existence of a match, not its generosity. This is consistent with behavioral theories emphasizing psychological framing over economic maximization.\n\n```{python}\n#| label: match-ratio-differences\n#| echo: true\n\n# 1. Direct from data\ngroup_1to1 = data[(data[\"treatment\"] == 1) & (data[\"ratio2\"] == 0) & (data[\"ratio3\"] == 0)][\"gave\"].dropna()\ngroup_2to1 = data[data[\"ratio2\"] == 1][\"gave\"].dropna()\ngroup_3to1 = data[data[\"ratio3\"] == 1][\"gave\"].dropna()\n\nmean_1to1 = group_1to1.mean()\nmean_2to1 = group_2to1.mean()\nmean_3to1 = group_3to1.mean()\n\n# Differences in means\ndiff_2vs1 = mean_2to1 - mean_1to1\ndiff_3vs2 = mean_3to1 - mean_2to1\n\nprint(\"Direct from data:\")\nprint(f\"2:1 - 1:1 difference: {diff_2vs1:.4f}\")\nprint(f\"3:1 - 2:1 difference: {diff_3vs2:.4f}\")\n\n# 2. From regression coefficients\n# Already created in earlier code: model_ratios (regression with ratio1, ratio2, ratio3 - 1)\ncoef_1to1 = model_ratios.params[\"ratio1\"]\ncoef_2to1 = model_ratios.params[\"ratio2\"]\ncoef_3to1 = model_ratios.params[\"ratio3\"]\n\n# Differences in fitted coefficients\nreg_diff_2vs1 = coef_2to1 - coef_1to1\nreg_diff_3vs2 = coef_3to1 - coef_2to1\n\nprint(\"\\nFrom regression coefficients:\")\nprint(f\"2:1 - 1:1 difference: {reg_diff_2vs1:.4f}\")\nprint(f\"3:1 - 2:1 difference: {reg_diff_3vs2:.4f}\")\n```\nWe compared response rates between the three match ratio conditions (1:1, 2:1, 3:1) using two methods: directly from the data and from regression coefficients.\n\n- The difference in response rates between 2:1 and 1:1 was 0.0019 (or 0.19 percentage points)\n- The difference between 3:1 and 2:1 was just 0.0001 (0.01 percentage points)\n\nThese results are not only tiny in magnitude, but also statistically insignificant (as shown in earlier regressions and t-tests).\n\nThese findings strongly support the authors’ point that while any match offer increases giving, the size of the match does not matter. Donors seem to respond to the existence of a match — perhaps as a sign of legitimacy, urgency, or impact — but increasing the match ratio from 1:1 to 3:1 does not meaningfully increase participation.\n\nThis is a powerful insight for fundraisers: you don’t need to offer huge matches to drive behavior. Even modest matching incentives are sufficient to unlock generosity.\n\n### Size of Charitable Contribution\n\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n```{python}\n#| label: ttest-reg-amount\n#| echo: true\n\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# Drop missing values\namount_treat = data[data[\"treatment\"] == 1][\"amount\"].dropna()\namount_ctrl = data[data[\"treatment\"] == 0][\"amount\"].dropna()\n\n# T-test (two-sample, unequal variance)\nt_stat, p_val = stats.ttest_ind(amount_treat, amount_ctrl, equal_var=False)\n\nprint(\"T-Test: Donation Amount by Treatment Status\")\nprint(f\"Mean (Treatment): {amount_treat.mean():.4f}\")\nprint(f\"Mean (Control):   {amount_ctrl.mean():.4f}\")\nprint(f\"t-statistic: {t_stat:.4f}\")\nprint(f\"p-value:     {p_val:.4f}\")\n\n# Regression: amount ~ treatment\nmodel_amt = smf.ols(\"amount ~ treatment\", data=data).fit()\n\nprint(\"\\nLinear Regression: Donation Amount on Treatment\")\nprint(model_amt.summary().tables[1])\n```\n\nWe tested whether being offered a matching donation increases the average donation amount (intensive margin), using both a t-test and a linear regression.\n\n- The treatment group gave more on average than the control group\n- The difference was approximately $0.15\n- However, this difference was not statistically significant at the 5% level (p ≈ 0.06)\n\nWhile matching offers clearly increase the likelihood of donating, their effect on the amount given is less conclusive. The data suggest a positive trend, but it falls just short of conventional significance. The match offer is a strong motivator for participation (extensive margin), but its influence on donation size (intensive margin) is weaker and more variable.\n\nIn short: matching offers bring more people in, but they may not substantially increase how much each person gives.\n\n```{python}\n#| label: regression-amount-givers-only\n#| echo: true\n\n# Subset to only donors\ndonors = data[data[\"gave\"] == 1].copy()\n\n# Run regression on donation amount\nimport statsmodels.formula.api as smf\nmodel_donors = smf.ols(\"amount ~ treatment\", data=donors).fit()\n\n# Show results\nprint(\"Regression: Donation Amount on Treatment (among donors only)\")\nprint(model_donors.summary().tables[1])\n\n```\nWe now focus on those who actually made a donation, to see if the treatment influenced how much people gave, once they decided to donate.\n\nThe regression shows that donors in the treatment group gave slightly less (about $1.67) than those in the control group — but this difference is not statistically significant (p = 0.561).\n\nThis result suggests that while match offers successfully increase the number of people who give, they do not increase — and may slightly decrease — the amount given by each donor.\n\nImportantly, because we are conditioning on a post-treatment variable (`gave == 1`), we cannot interpret this coefficient causally. The treatment may have changed who donates, and those people may have different baseline donation levels. These results confirm the central insight of the paper, i.e. matching gifts are effective at increasing participation, but not at increasing donation size.\n\nIn other words, matching offers are a great tool to broaden the donor base, but they don’t necessarily make each donor more generous.\n\n```{python}\n#| label: donation-histogram-fixed\n#| echo: true\n#| fig-cap: \"Donation Amounts Among Donors (Control vs. Treatment)\"\n#| fig-align: center\n\nimport matplotlib.pyplot as plt\n\n# Subset to donors only\ndonors = data[data[\"gave\"] == 1].copy()\n\n# Debug: Check treatment value counts\nprint(\"Donor treatment group distribution:\")\nprint(donors[\"treatment\"].value_counts())\n\n# Make sure these are correct\ndonors_treat = donors[donors[\"treatment\"] == 1][\"amount\"]\ndonors_ctrl = donors[donors[\"treatment\"] == 0][\"amount\"]\n\n# Means\nmean_treat = donors_treat.mean()\nmean_ctrl = donors_ctrl.mean()\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Control group\naxes[0].hist(donors_ctrl, bins=30, color='skyblue', edgecolor='white')\naxes[0].axvline(mean_ctrl, color='red', linestyle='dashed', linewidth=2)\naxes[0].set_title(\"Control Group\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Number of Donors\")\naxes[0].legend([f\"Mean = {mean_ctrl:.2f}\"], loc='upper right')\n\n# Treatment group\naxes[1].hist(donors_treat, bins=30, color='lightgreen', edgecolor='white')\naxes[1].axvline(mean_treat, color='red', linestyle='dashed', linewidth=2)\naxes[1].set_title(\"Treatment Group\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend([f\"Mean = {mean_treat:.2f}\"], loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\n```\nThis plot compares the distribution of donation amounts among those who donated, split by treatment status.\n\n- Both distributions are right-skewed — many donors give smaller amounts, with a few larger donations.\n- The red dashed lines indicate the average donation in each group:\n  - Control group mean ≈ $45.54\n  - Treatment group mean ≈ $43.87\n\nWhile the control group donated slightly more on average than the treatment group, this difference is not statistically significant, as we saw in the conditional regression earlier.\n\nThis supports the paper’s conclusion that: Matching offers increase participation, but not donation size per donor.\n\nThe presence of a match encourages more people to give — but once they’re in, it doesn’t meaningfully change how much they give. The effectiveness of the match appears to work through activation, not through amplification.\n\n\n## Simulation Experiment\n\nAs a reminder of how the t-statistic \"works,\" in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\n\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. \n\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size  is Bernoulli with probability p=0.022 that a donation is made.\n\n### Law of Large Numbers\n\n```{python}\n#| label: simulation-lln\n#| echo: true\n#| fig-cap: \"Cumulative Average of Differences in Means: Simulated LLN\"\n#| fig-align: center\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Simulation parameters\nn_sim = 10_000\ncontrol_p = 0.018\ntreatment_p = 0.022\n\n# Simulate draws\ncontrol_samples = np.random.binomial(1, control_p, size=(n_sim,))\ntreatment_samples = np.random.binomial(1, treatment_p, size=(n_sim,))\n\n# Compute vector of differences\ndiffs = treatment_samples - control_samples\n\n# Cumulative average of differences\ncumulative_avg = np.cumsum(diffs) / np.arange(1, n_sim + 1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label=\"Cumulative Average Difference\", color=\"blue\")\nplt.axhline(treatment_p - control_p, color=\"red\", linestyle=\"--\", label=\"True Difference (0.004)\")\nplt.title(\"Cumulative Average of Simulated Differences in Giving Rates\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average (Treatment - Control)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n```\n\nEach simulated draw represents a pair of Bernoulli outcomes: one from a population with a 1.8% donation rate (control), and one with a 2.2% rate (treatment). We repeat this 10,000 times, then compute the cumulative average difference in outcomes.\n\n- Early on, the average difference fluctuates due to randomness\n- As the number of simulations increases, the average stabilizes near 0.004 — the true population difference\n- This illustrates the LLN: the sample average converges to the expected value as sample size grows\n\nIt shows that even though early estimates may be noisy, with enough data, we get very close to the truth. \n\n### Central Limit Theorem\n\n```{python}\n#| label: clt-histograms\n#| echo: true\n#| fig-cap: \"Central Limit Theorem: Sampling Distributions of Differences at Varying Sample Sizes\"\n#| fig-align: center\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Parameters\nn_simulations = 1000\nsample_sizes = [50, 200, 500, 1000]\np_control = 0.018\np_treatment = 0.022\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(n_simulations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        diffs.append(diff)\n    \n    axes[i].hist(diffs, bins=30, color='lightblue', edgecolor='black')\n    axes[i].axvline(x=0, color='red', linestyle='--', label='Zero')\n    axes[i].axvline(x=0.004, color='green', linestyle='--', label='True Diff = 0.004')\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Mean Difference\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n```\n\nThis simulation demonstrates the Central Limit Theorem (CLT) by comparing the distribution of mean differences (treatment - control) at various sample sizes.\n\nEach plot shows the sampling distribution of mean differences across 1,000 simulated samples, at sample sizes of 50, 200, 500, and 1,000. For each simulation, we:\n- Drew `n` Bernoulli observations from a treatment group (\\( p = 0.022 \\)) and a control group (\\( p = 0.018 \\))\n- Calculated the mean difference in donation rates\n- Plotted the resulting distribution\n\nTwo reference lines are drawn on each histogram:\n- Red dashed line: Zero (null hypothesis of no effect)\n- Green dashed line: True difference (0.004)\n\nKey Insights:\n1. As sample size increases:\n   - The distribution becomes narrower (less variance)\n   - It becomes more symmetric and bell-shaped\n   - It centers more closely around the true effect of 0.004\n\n2. For small sample sizes (n = 50):\n   - The distribution is wide and noisy\n   - Zero lies near the center, so we likely wouldn’t reject the null in most replications\n\n3. For larger samples (n = 500 or 1000):\n   - The distribution is much tighter\n   - Zero lies in the tail, meaning the treatment effect would be detected as statistically significant more often\n\nThis simulation highlights how sample size affects our ability to detect treatment effects. Small samples yield imprecise, noisy estimates, even when a real effect exists. As sample size increases, our estimates:\n- Become more precise\n- Approach the true treatment effect\n- Follow the normal distribution, enabling the use of t-tests and confidence intervals\n\nIn short, the CLT tells us that with enough data, we can rely on the sample mean as a good estimator — and this explains why large-scale experiments like Karlan & List's produce reliable and robust results.\n\n\n\n\n\n","srcMarkdownNoYaml":"\n\n\n## Introduction\n\nDean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the _American Economic Review_ in 2007. The article and supporting data are available from the [AEA website](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774) and from Innovations for Poverty Action as part of [Harvard's Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27853&version=4.2).\n\nIn the early 2000s, economists Dean Karlan and John List partnered with a politically oriented nonprofit organization to run a large-scale natural field experiment on charitable giving. The organization sent out over 50,000 fundraising letters to prior donors, randomly assigning each recipient to a different version of the letter.\n\nThe central research question was: Does offering a matching grant increase the likelihood and amount of charitable donations?\n\nTo answer this, recipients were randomly assigned to:\n- A control group that received a standard appeal,\n- A treatment group that was told a generous donor would match their gift at one of several ratios: 1:1, 2:1, or 3:1.\n\nOther variables were randomized as well:\n- The maximum amount of the matching grant (\\$25k, \\$50k, \\$100k, or unstated),\n- The suggested donation amount, based on their previous giving.\n\nThis rich experimental design allows for credible causal inference on how donors respond to match offers — both in whether they give at all (extensive margin) and how much they give (intensive margin). It’s one of the first large-scale field experiments to rigorously test “price” sensitivity in fundraising using real behavior and real money.\n\nThis project seeks to replicate their results.\n\n\n## Data\n\n### Description\n\n```{python}\n#| label: load-and-describe-data\n#| echo: true\n#| fig-cap-location: bottom\n\nimport pandas as pd\n\n# Load the actual dataset you just uploaded\ndata = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Preview the first few rows\nprint(data.head())\n```\n\n```{python}\nprint(data.shape)\nprint(data.info())\n```\n\n```{python}\n#| label: data-summary\n#| echo: true\n\n# Summary statistics for all variables\ndata.describe()\n\n```\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n| Variable             | Description                                                         |\n|----------------------|---------------------------------------------------------------------|\n| `treatment`          | Treatment                                                           |\n| `control`            | Control                                                             |\n| `ratio`              | Match ratio                                                         |\n| `ratio2`             | 2:1 match ratio                                                     |\n| `ratio3`             | 3:1 match ratio                                                     |\n| `size`               | Match threshold                                                     |\n| `size25`             | \\$25,000 match threshold                                            |\n| `size50`             | \\$50,000 match threshold                                            |\n| `size100`            | \\$100,000 match threshold                                           |\n| `sizeno`             | Unstated match threshold                                            |\n| `ask`                | Suggested donation amount                                           |\n| `askd1`              | Suggested donation was highest previous contribution                |\n| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |\n| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |\n| `ask1`               | Highest previous contribution (for suggestion)                      |\n| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |\n| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |\n| `amount`             | Dollars given                                                       |\n| `gave`               | Gave anything                                                       |\n| `amountchange`       | Change in amount given                                              |\n| `hpa`                | Highest previous contribution                                       |\n| `ltmedmra`           | Small prior donor: last gift was less than median \\$35              |\n| `freq`               | Number of prior donations                                           |\n| `years`              | Number of years since initial donation                              |\n| `year5`              | At least 5 years since initial donation                             |\n| `mrm2`               | Number of months since last donation                                |\n| `dormant`            | Already donated in 2005                                             |\n| `female`             | Female                                                              |\n| `couple`             | Couple                                                              |\n| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |\n| `nonlit`             | Nonlitigation                                                       |\n| `cases`              | Court cases from state in 2004-5 in which organization was involved |\n| `statecnt`           | Percent of sample from state                                        |\n| `stateresponse`      | Proportion of sample from the state who gave                        |\n| `stateresponset`     | Proportion of treated sample from the state who gave                |\n| `stateresponsec`     | Proportion of control sample from the state who gave                |\n| `stateresponsetminc` | stateresponset - stateresponsec                                     |\n| `perbush`            | State vote share for Bush                                           |\n| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |\n| `red0`               | Red state                                                           |\n| `blue0`              | Blue state                                                          |\n| `redcty`             | Red county                                                          |\n| `bluecty`            | Blue county                                                         |\n| `pwhite`             | Proportion white within zip code                                    |\n| `pblack`             | Proportion black within zip code                                    |\n| `page18_39`          | Proportion age 18-39 within zip code                                |\n| `ave_hh_sz`          | Average household size within zip code                              |\n| `median_hhincome`    | Median household income within zip code                             |\n| `powner`             | Proportion house owner within zip code                              |\n| `psch_atlstba`       | Proportion who finished college within zip code                     |\n| `pop_propurban`      | Proportion of population urban within zip code                      |\n\n::::\n\n\n### Balance Test \n\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n```{python}\n#| label: balance-ttests\n#| echo: true\n\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# Balance test on observed covariates\nbalance_vars = ['ask1', 'years', 'female', 'median_hhincome','mrm2','couple']\n\nfor var in balance_vars:\n    treated = data.loc[data['treatment'] == 1, var].dropna()\n    control = data.loc[data['treatment'] == 0, var].dropna()\n\n    x1, x2 = treated.mean(), control.mean()\n    s1, s2 = treated.std(), control.std()\n    n1, n2 = len(treated), len(control)\n\n    # Manual t-statistic (pooled standard error)\n    se = np.sqrt((s1**2 / n1) + (s2**2 / n2))\n    t_stat = (x1 - x2) / se\n    df = min(n1, n2) - 1\n    p_val = 2 * (1 - stats.t.cdf(abs(t_stat), df))\n\n    print(f\"\\nVariable: {var}\")\n    print(f\"Manual t-test: t = {t_stat:.4f}, p = {p_val:.4f}\")\n\n    # Linear regression\n    model = smf.ols(f\"{var} ~ treatment\", data=data).fit()\n    coef = model.params['treatment']\n    p = model.pvalues['treatment']\n    print(f\"Regression: Coef = {coef:.4f}, p = {p:.4f}\")\n\n```\n\nThe balance tests above check whether the treatment and control groups differ before the intervention on observable characteristics.\nKey insights include:\n- All p-values are greater than 0.05, meaning no statistically significant differences between groups\n- T-test statistics and regression coefficients are numerically equivalent, confirming the equivalence of these two methods in this context\n\nThis result is consistent with the claim that random assignment was successful — any differences observed in donation behavior later on can be attributed to the treatment itself, not pre-existing group differences.\n\n\n## Experimental Results\n\n### Charitable Contribution Made\n\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation. \n\nTo evaluate the effect of matched donations on the likelihood of giving (i.e., the response rate), we first compare the proportion of people who donated in the treatment vs. control groups.\n\nBelow is a simple bar plot showing the share of donors in each group.\n\n```{python}\n#| label: barplot-response-rate\n#| fig-cap: \"Proportion of Donors by Treatment Status\"\n#| fig-align: center\n\nimport matplotlib.pyplot as plt\n\n# Calculate donation rates\ndonation_rates = data.groupby(\"treatment\")[\"gave\"].mean()\nlabels = [\"Control\", \"Treatment\"]\n\n# Create bar plot\nplt.figure(figsize=(6, 4))\nbars = plt.bar(labels, donation_rates, color=[\"skyblue\", \"lightgreen\"])\nplt.title(\"Proportion of Donors by Treatment Status\")\nplt.ylabel(\"Proportion Who Donated\")\nplt.ylim(0, max(donation_rates)*1.2)\n\n# Annotate bar heights\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.0005, f\"{yval:.3f}\", ha=\"center\", va=\"bottom\")\n\nplt.tight_layout()\nplt.show()\n```\nThe bar plot shows that the treatment group — those who received a matching donation offer — had a higher donation rate than the control group.\n\nThis suggests that matched donations may be effective at increasing the likelihood of contributing. We'll test the statistical significance of this next using t-tests and regression.\n\n\nWe now formally test whether the treatment group was significantly more likely to donate than the control group. \nWe use two methods:\n1. A manual t-test, comparing donation rates across groups  \n2. A linear regression of gave on treatment\n\nThese approaches should return numerically identical p-values and treatment effect estimates.\n\n```{python}\n#| label: ttest-and-reg-gave\n#| echo: true\n\nimport numpy as np\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# Prepare data\ngave_treat = data[data[\"treatment\"] == 1][\"gave\"].dropna()\ngave_ctrl = data[data[\"treatment\"] == 0][\"gave\"].dropna()\n\n# Group statistics\nmean_treat = gave_treat.mean()\nmean_ctrl = gave_ctrl.mean()\nstd_treat = gave_treat.std()\nstd_ctrl = gave_ctrl.std()\nn_treat = len(gave_treat)\nn_ctrl = len(gave_ctrl)\n\n# Manual t-test\nse_diff = np.sqrt((std_treat**2 / n_treat) + (std_ctrl**2 / n_ctrl))\nt_stat = (mean_treat - mean_ctrl) / se_diff\ndf = min(n_treat, n_ctrl) - 1\np_val = 2 * (1 - stats.t.cdf(abs(t_stat), df))\n\nprint(\"Manual T-Test:\")\nprint(f\"  Mean (Treatment): {mean_treat:.4f}\")\nprint(f\"  Mean (Control):   {mean_ctrl:.4f}\")\nprint(f\"  t-statistic: {t_stat:.4f}\")\nprint(f\"  p-value:     {p_val:.4f}\")\n\n# Regression: gave ~ treatment\nmodel = smf.ols(\"gave ~ treatment\", data=data).fit()\n\nprint(\"\\nLinear Regression:\")\nprint(model.summary().tables[1])\n```\nTo test whether offering a matched donation increases the likelihood of giving, we compared donation rates between the treatment group (who received a match offer) and the control group (who did not).\n\n- The t-test showed a statistically significant difference in donation rates: those who received a match offer were more likely to give.\n- The regression confirmed this result. The coefficient on `treatment` indicates that being offered a match increased the probability of giving by about 0.42 percentage points.\n\nWhile the difference may seem small in absolute terms, it is meaningful given the scale of the experiment (over 50,000 individuals). The fact that such a low-cost intervention can move behavior at all is powerful.\n\nThis finding suggests that people are more willing to give when they believe their donation is matched — even if the actual benefit is external. It aligns with the idea that charitable behavior is influenced not just by intrinsic values, but also by nudges, framing, and social cues. The presence of a matching offer makes the donation feel more impactful, and that perception increases participation.\n\n\n```{python}\n#| label: probit-gave-treatment\n#| echo: true\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# Run probit regression\nprobit_model = smf.probit(\"gave ~ treatment\", data=data).fit()\n\n# Print coefficient table\nprint(probit_model.summary())\n\n# Compute marginal effects at the mean (default)\nmfx = probit_model.get_margeff()\nprint(mfx.summary())\n```\n\n\nThe estimated probit coefficient on `treatment` is 0.0868, which is statistically significant (p = 0.002). The marginal effect is approximately 0.0043, meaning that receiving a matching offer increases the probability of donating by 0.43 percentage points.\n\nThis result is nearly identical to what we saw in the linear regression and t-test earlier. It reinforces the key takeaway:  \n**A simple offer to match donations meaningfully increases participation in charitable giving.**\n\nThe probit model is a better fit for binary outcomes like donation/no donation, but it leads to the same substantive conclusion — matching offers are effective nudges.\n\n\n### Differences between Match Rates\n\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n```{python}\n#| label: ttests-match-ratios\n#| echo: true\n\nfrom scipy import stats\n\n# Define each group explicitly\ngroup_1to1 = data[(data[\"treatment\"] == 1) & (data[\"ratio2\"] == 0) & (data[\"ratio3\"] == 0)][\"gave\"].dropna()\ngroup_2to1 = data[data[\"ratio2\"] == 1][\"gave\"].dropna()\ngroup_3to1 = data[data[\"ratio3\"] == 1][\"gave\"].dropna()\n\n# Helper function to run and print t-test\ndef ttest_compare(label1, g1, label2, g2):\n    t_stat, p_val = stats.ttest_ind(g1, g2, equal_var=False)\n    print(f\"{label1} vs {label2}: t = {t_stat:.4f}, p = {p_val:.4f}\")\n\n# Run t-tests\nttest_compare(\"2:1\", group_2to1, \"1:1\", group_1to1)\nttest_compare(\"3:1\", group_3to1, \"1:1\", group_1to1)\nttest_compare(\"3:1\", group_3to1, \"2:1\", group_2to1)\n\n```\nWe tested whether increasing the match ratio (e.g., from 1:1 to 2:1 or 3:1) had any significant effect on the likelihood of giving. This directly addresses the authors' comment on page 8 of Karlan & List (2007) that \"*the figures suggest that increasing the match rate does little to increase the response rate.*\"\n\nOur t-tests confirm this conclusion:\n\n- The difference in donation rates between 2:1 and 1:1 was not statistically significant (p = 0.33)\n- The difference between 3:1 and 1:1 was also not significant (p = 0.31)\n- Even 3:1 vs 2:1 produced virtually no difference (p = 0.96)\n\nThese results suggest that it’s not the size of the match that matters, but simply the presence of a match offer. The behavior of donors appears consistent with psychological nudges — once motivated by a match, increasing its value doesn’t further increase giving.\n\n```{python}\n#| label: regression-ratio-fix\n#| echo: true\n\n# Create ratio1 (1:1 match dummy)\ndata[\"ratio1\"] = ((data[\"treatment\"] == 1) & (data[\"ratio2\"] == 0) & (data[\"ratio3\"] == 0)).astype(int)\n\n# Filter treatment group\ntreat_data = data[data[\"treatment\"] == 1].copy()\n\n# Fix: Drop intercept explicitly\nmodel_ratios = smf.ols(\"gave ~ ratio1 + ratio2 + ratio3 - 1\", data=treat_data).fit()\nprint(\"Regression using ratio dummies (no intercept):\")\nprint(model_ratios.summary().tables[1])\n```\n\nWe regressed the binary outcome `gave` on dummy variables for each match ratio level — 1:1, 2:1, and 3:1 — using a no-intercept model. This setup allows each coefficient to directly represent the mean response rate for that ratio group.\n\n- The response rate under a 1:1 match was 2.07%\n- Under 2:1, it was 2.26%\n- Under 3:1, it was 2.27%\n\nWhile all coefficients are statistically significant due to the large sample size, the differences between them are extremely small. Thus, the size of the match doesn’t matter — donors appear to respond to the existence of a match, not its generosity. This is consistent with behavioral theories emphasizing psychological framing over economic maximization.\n\n```{python}\n#| label: match-ratio-differences\n#| echo: true\n\n# 1. Direct from data\ngroup_1to1 = data[(data[\"treatment\"] == 1) & (data[\"ratio2\"] == 0) & (data[\"ratio3\"] == 0)][\"gave\"].dropna()\ngroup_2to1 = data[data[\"ratio2\"] == 1][\"gave\"].dropna()\ngroup_3to1 = data[data[\"ratio3\"] == 1][\"gave\"].dropna()\n\nmean_1to1 = group_1to1.mean()\nmean_2to1 = group_2to1.mean()\nmean_3to1 = group_3to1.mean()\n\n# Differences in means\ndiff_2vs1 = mean_2to1 - mean_1to1\ndiff_3vs2 = mean_3to1 - mean_2to1\n\nprint(\"Direct from data:\")\nprint(f\"2:1 - 1:1 difference: {diff_2vs1:.4f}\")\nprint(f\"3:1 - 2:1 difference: {diff_3vs2:.4f}\")\n\n# 2. From regression coefficients\n# Already created in earlier code: model_ratios (regression with ratio1, ratio2, ratio3 - 1)\ncoef_1to1 = model_ratios.params[\"ratio1\"]\ncoef_2to1 = model_ratios.params[\"ratio2\"]\ncoef_3to1 = model_ratios.params[\"ratio3\"]\n\n# Differences in fitted coefficients\nreg_diff_2vs1 = coef_2to1 - coef_1to1\nreg_diff_3vs2 = coef_3to1 - coef_2to1\n\nprint(\"\\nFrom regression coefficients:\")\nprint(f\"2:1 - 1:1 difference: {reg_diff_2vs1:.4f}\")\nprint(f\"3:1 - 2:1 difference: {reg_diff_3vs2:.4f}\")\n```\nWe compared response rates between the three match ratio conditions (1:1, 2:1, 3:1) using two methods: directly from the data and from regression coefficients.\n\n- The difference in response rates between 2:1 and 1:1 was 0.0019 (or 0.19 percentage points)\n- The difference between 3:1 and 2:1 was just 0.0001 (0.01 percentage points)\n\nThese results are not only tiny in magnitude, but also statistically insignificant (as shown in earlier regressions and t-tests).\n\nThese findings strongly support the authors’ point that while any match offer increases giving, the size of the match does not matter. Donors seem to respond to the existence of a match — perhaps as a sign of legitimacy, urgency, or impact — but increasing the match ratio from 1:1 to 3:1 does not meaningfully increase participation.\n\nThis is a powerful insight for fundraisers: you don’t need to offer huge matches to drive behavior. Even modest matching incentives are sufficient to unlock generosity.\n\n### Size of Charitable Contribution\n\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n```{python}\n#| label: ttest-reg-amount\n#| echo: true\n\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n\n# Drop missing values\namount_treat = data[data[\"treatment\"] == 1][\"amount\"].dropna()\namount_ctrl = data[data[\"treatment\"] == 0][\"amount\"].dropna()\n\n# T-test (two-sample, unequal variance)\nt_stat, p_val = stats.ttest_ind(amount_treat, amount_ctrl, equal_var=False)\n\nprint(\"T-Test: Donation Amount by Treatment Status\")\nprint(f\"Mean (Treatment): {amount_treat.mean():.4f}\")\nprint(f\"Mean (Control):   {amount_ctrl.mean():.4f}\")\nprint(f\"t-statistic: {t_stat:.4f}\")\nprint(f\"p-value:     {p_val:.4f}\")\n\n# Regression: amount ~ treatment\nmodel_amt = smf.ols(\"amount ~ treatment\", data=data).fit()\n\nprint(\"\\nLinear Regression: Donation Amount on Treatment\")\nprint(model_amt.summary().tables[1])\n```\n\nWe tested whether being offered a matching donation increases the average donation amount (intensive margin), using both a t-test and a linear regression.\n\n- The treatment group gave more on average than the control group\n- The difference was approximately $0.15\n- However, this difference was not statistically significant at the 5% level (p ≈ 0.06)\n\nWhile matching offers clearly increase the likelihood of donating, their effect on the amount given is less conclusive. The data suggest a positive trend, but it falls just short of conventional significance. The match offer is a strong motivator for participation (extensive margin), but its influence on donation size (intensive margin) is weaker and more variable.\n\nIn short: matching offers bring more people in, but they may not substantially increase how much each person gives.\n\n```{python}\n#| label: regression-amount-givers-only\n#| echo: true\n\n# Subset to only donors\ndonors = data[data[\"gave\"] == 1].copy()\n\n# Run regression on donation amount\nimport statsmodels.formula.api as smf\nmodel_donors = smf.ols(\"amount ~ treatment\", data=donors).fit()\n\n# Show results\nprint(\"Regression: Donation Amount on Treatment (among donors only)\")\nprint(model_donors.summary().tables[1])\n\n```\nWe now focus on those who actually made a donation, to see if the treatment influenced how much people gave, once they decided to donate.\n\nThe regression shows that donors in the treatment group gave slightly less (about $1.67) than those in the control group — but this difference is not statistically significant (p = 0.561).\n\nThis result suggests that while match offers successfully increase the number of people who give, they do not increase — and may slightly decrease — the amount given by each donor.\n\nImportantly, because we are conditioning on a post-treatment variable (`gave == 1`), we cannot interpret this coefficient causally. The treatment may have changed who donates, and those people may have different baseline donation levels. These results confirm the central insight of the paper, i.e. matching gifts are effective at increasing participation, but not at increasing donation size.\n\nIn other words, matching offers are a great tool to broaden the donor base, but they don’t necessarily make each donor more generous.\n\n```{python}\n#| label: donation-histogram-fixed\n#| echo: true\n#| fig-cap: \"Donation Amounts Among Donors (Control vs. Treatment)\"\n#| fig-align: center\n\nimport matplotlib.pyplot as plt\n\n# Subset to donors only\ndonors = data[data[\"gave\"] == 1].copy()\n\n# Debug: Check treatment value counts\nprint(\"Donor treatment group distribution:\")\nprint(donors[\"treatment\"].value_counts())\n\n# Make sure these are correct\ndonors_treat = donors[donors[\"treatment\"] == 1][\"amount\"]\ndonors_ctrl = donors[donors[\"treatment\"] == 0][\"amount\"]\n\n# Means\nmean_treat = donors_treat.mean()\nmean_ctrl = donors_ctrl.mean()\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# Control group\naxes[0].hist(donors_ctrl, bins=30, color='skyblue', edgecolor='white')\naxes[0].axvline(mean_ctrl, color='red', linestyle='dashed', linewidth=2)\naxes[0].set_title(\"Control Group\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Number of Donors\")\naxes[0].legend([f\"Mean = {mean_ctrl:.2f}\"], loc='upper right')\n\n# Treatment group\naxes[1].hist(donors_treat, bins=30, color='lightgreen', edgecolor='white')\naxes[1].axvline(mean_treat, color='red', linestyle='dashed', linewidth=2)\naxes[1].set_title(\"Treatment Group\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend([f\"Mean = {mean_treat:.2f}\"], loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\n```\nThis plot compares the distribution of donation amounts among those who donated, split by treatment status.\n\n- Both distributions are right-skewed — many donors give smaller amounts, with a few larger donations.\n- The red dashed lines indicate the average donation in each group:\n  - Control group mean ≈ $45.54\n  - Treatment group mean ≈ $43.87\n\nWhile the control group donated slightly more on average than the treatment group, this difference is not statistically significant, as we saw in the conditional regression earlier.\n\nThis supports the paper’s conclusion that: Matching offers increase participation, but not donation size per donor.\n\nThe presence of a match encourages more people to give — but once they’re in, it doesn’t meaningfully change how much they give. The effectiveness of the match appears to work through activation, not through amplification.\n\n\n## Simulation Experiment\n\nAs a reminder of how the t-statistic \"works,\" in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\n\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. \n\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size  is Bernoulli with probability p=0.022 that a donation is made.\n\n### Law of Large Numbers\n\n```{python}\n#| label: simulation-lln\n#| echo: true\n#| fig-cap: \"Cumulative Average of Differences in Means: Simulated LLN\"\n#| fig-align: center\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Simulation parameters\nn_sim = 10_000\ncontrol_p = 0.018\ntreatment_p = 0.022\n\n# Simulate draws\ncontrol_samples = np.random.binomial(1, control_p, size=(n_sim,))\ntreatment_samples = np.random.binomial(1, treatment_p, size=(n_sim,))\n\n# Compute vector of differences\ndiffs = treatment_samples - control_samples\n\n# Cumulative average of differences\ncumulative_avg = np.cumsum(diffs) / np.arange(1, n_sim + 1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label=\"Cumulative Average Difference\", color=\"blue\")\nplt.axhline(treatment_p - control_p, color=\"red\", linestyle=\"--\", label=\"True Difference (0.004)\")\nplt.title(\"Cumulative Average of Simulated Differences in Giving Rates\")\nplt.xlabel(\"Number of Simulations\")\nplt.ylabel(\"Cumulative Average (Treatment - Control)\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n```\n\nEach simulated draw represents a pair of Bernoulli outcomes: one from a population with a 1.8% donation rate (control), and one with a 2.2% rate (treatment). We repeat this 10,000 times, then compute the cumulative average difference in outcomes.\n\n- Early on, the average difference fluctuates due to randomness\n- As the number of simulations increases, the average stabilizes near 0.004 — the true population difference\n- This illustrates the LLN: the sample average converges to the expected value as sample size grows\n\nIt shows that even though early estimates may be noisy, with enough data, we get very close to the truth. \n\n### Central Limit Theorem\n\n```{python}\n#| label: clt-histograms\n#| echo: true\n#| fig-cap: \"Central Limit Theorem: Sampling Distributions of Differences at Varying Sample Sizes\"\n#| fig-align: center\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed for reproducibility\nnp.random.seed(42)\n\n# Parameters\nn_simulations = 1000\nsample_sizes = [50, 200, 500, 1000]\np_control = 0.018\np_treatment = 0.022\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(n_simulations):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        diffs.append(diff)\n    \n    axes[i].hist(diffs, bins=30, color='lightblue', edgecolor='black')\n    axes[i].axvline(x=0, color='red', linestyle='--', label='Zero')\n    axes[i].axvline(x=0.004, color='green', linestyle='--', label='True Diff = 0.004')\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Mean Difference\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n```\n\nThis simulation demonstrates the Central Limit Theorem (CLT) by comparing the distribution of mean differences (treatment - control) at various sample sizes.\n\nEach plot shows the sampling distribution of mean differences across 1,000 simulated samples, at sample sizes of 50, 200, 500, and 1,000. For each simulation, we:\n- Drew `n` Bernoulli observations from a treatment group (\\( p = 0.022 \\)) and a control group (\\( p = 0.018 \\))\n- Calculated the mean difference in donation rates\n- Plotted the resulting distribution\n\nTwo reference lines are drawn on each histogram:\n- Red dashed line: Zero (null hypothesis of no effect)\n- Green dashed line: True difference (0.004)\n\nKey Insights:\n1. As sample size increases:\n   - The distribution becomes narrower (less variance)\n   - It becomes more symmetric and bell-shaped\n   - It centers more closely around the true effect of 0.004\n\n2. For small sample sizes (n = 50):\n   - The distribution is wide and noisy\n   - Zero lies near the center, so we likely wouldn’t reject the null in most replications\n\n3. For larger samples (n = 500 or 1000):\n   - The distribution is much tighter\n   - Zero lies in the tail, meaning the treatment effect would be detected as statistically significant more often\n\nThis simulation highlights how sample size affects our ability to detect treatment effects. Small samples yield imprecise, noisy estimates, even when a real effect exists. As sample size increases, our estimates:\n- Become more precise\n- Approach the true treatment effect\n- Follow the normal distribution, enabling the use of t-tests and confidence intervals\n\nIn short, the CLT tells us that with enough data, we can rely on the sample mean as a good estimator — and this explains why large-scale experiments like Karlan & List's produce reliable and robust results.\n\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.5","theme":["cosmo","brand"],"title":"A Replication of Karlan and List (2007)","author":"Samiksha Shetty","date":"today","callout-appearance":"minimal"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}