{"title":"Poisson Regression Examples","markdown":{"yaml":{"title":"Poisson Regression Examples","author":"Samiksha Shetty","date":"today","callout-appearance":"minimal"},"headingText":"Blueprinty Case Study","containsRefs":false,"markdown":"\n\n\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n\n```{python}\nimport pandas as pd\nblueprinty_data = pd.read_csv(\"blueprinty.csv\")\nblueprinty_data.head()\n```\n\n```{python}\n# Compute mean number of patents for customers and non-customers\nmean_patents = blueprinty_data.groupby('iscustomer')['patents'].mean()\nmean_patents\n```\n\n```{python}\nimport matplotlib.pyplot as plt\n# Bin the number of patents into intervals of size 2\nblueprinty_data['patent_bin'] = pd.cut(\n    blueprinty_data['patents'],\n    bins=range(0, blueprinty_data['patents'].max() + 2, 2),\n    right=False\n)\n\n# Count number of firms in each bin by customer status\npatent_counts = blueprinty_data.groupby(['patent_bin', 'iscustomer']).size().unstack(fill_value=0)\n\n# Convert to row-wise proportions\npatent_props = patent_counts.div(patent_counts.sum(axis=1), axis=0)\n\n# Plot\npatent_props.plot(kind='bar', figsize=(8, 6), width=0.8, color=['#efc0fc', '#c0e861'])\nplt.title('Proportion of Firms by Patent Bin and Customer Status')\nplt.xlabel('Number of Patents (Binned)')\nplt.ylabel('Proportion of Firms')\nplt.xticks(rotation=45)\nplt.legend(title='Is Customer', labels=['No', 'Yes'])\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n```\n\n\nWe observe that **Blueprinty customers tend to have more patents** than non-customers, both on average and in their distribution.\n\n- The **average number of patents** is higher for customers (**4.13**) than for non-customers (**3.47**).\n- **Non-customers** are heavily concentrated in lower patent bins (e.g., `0–6` patents).\n- **Customers** appear more frequently in **higher patent bins**, such as `6–10` and `10–14`.\n- When proportions are normalized by group size, **customers are overrepresented** in higher patent brackets. In the `[12–14)` bin, customers even make up the majority.\n\nThis pattern suggests that firms using Blueprinty's software are **more successful in obtaining patents**, supporting the hypothesis that Blueprinty's product contributes to improved patenting outcomes.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n```{python}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Boxplot of age by customer status\nplt.figure(figsize=(7, 5))\nsns.boxplot(x='iscustomer', y='age', data=blueprinty_data, palette=[\"#efc0fc\", \"#c3ff63\"])\nplt.title(\"Firm Age by Blueprinty Customer Status\")\nplt.xlabel(\"Is Customer (0 = No, 1 = Yes)\")\nplt.ylabel(\"Firm Age (Years)\")\nplt.grid(True)\nplt.show()\n```\n#### Firm Age\n\nThe boxplot comparing firm ages by Blueprinty customer status reveals that **customers tend to be slightly older on average** than non-customers. \n\n- While the **median age** is somewhat higher for customers, the overall spread (**interquartile range**) is similar across both groups.  \n- This suggests that **more established firms** may be more likely to adopt Blueprinty's software.\n\n\n```{python}\n# Crosstab of customer status by region\nregion_counts = pd.crosstab(blueprinty_data['region'], blueprinty_data['iscustomer'])\n\n# Convert to proportions (row-wise)\nregion_props = region_counts.div(region_counts.sum(axis=1), axis=0)\n\n# Bar plot of customer proportions by region\nregion_props.plot(kind='bar', stacked=True, figsize=(8, 5), colormap='Set2')\nplt.title(\"Proportion of Customers by Region\")\nplt.ylabel(\"Proportion of Firms\")\nplt.xlabel(\"Region\")\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nregion_counts\n\n```\n#### Region\nThe stacked bar chart shows variation in Blueprinty adoption rates across regions:\n\n- The **Northeast** has a noticeably higher proportion of customers, with over **50%** of firms using Blueprinty.\n- In contrast, other regions like the **Midwest**, **Northwest**, **South**, and **Southwest** have much lower customer proportions — around **15–20%**.\n\nThis suggests stronger **regional presence** or **marketing penetration** in the Northeast, which could influence future outreach or sales strategy.\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nWe assume that the number of patents $Y_i$ for firm $i$ follows a Poisson distribution:\n\n$$\nY_i \\sim \\text{Poisson}(\\lambda)\n$$\n\nThe likelihood for a single observation is:\n\n$$\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nAnd the joint likelihood over $n$ independent firms is:\n\n$$\nL(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nTaking logs gives the log-likelihood function:\n\n$$\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n$$\n\n\n```{python}\nimport numpy as np\nfrom scipy.special import factorial\n\n# Define Poisson log-likelihood function\ndef poisson_log_likelihood(lmbda, y):\n    if lmbda <= 0:\n        return -np.inf  # log-likelihood undefined for non-positive lambda\n    return np.sum(y * np.log(lmbda) - lmbda - np.log(factorial(y)))\n\nY = blueprinty_data['patents'].values\npoisson_log_likelihood(lmbda=4.0, y=Y)  # Example evaluation at lambda = 4.0\n\n```\n\nThis number by itself isn’t “good” or “bad” — it's just **one point on the log-likelihood curve**. The goal now is to find the value of $\\lambda$ that **maximizes this log-likelihood** — that is, the **Maximum Likelihood Estimate (MLE)** of $\\lambda$.\n\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Observed patent data\nY = blueprinty_data['patents'].values\n\n# Lambda range to evaluate\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_log_likelihood(lmbda, Y) for lmbda in lambda_vals]\n\n# Plot log-likelihood curve\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='green')\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.axvline(x=np.mean(Y), color='red', linestyle='--', label=f\"Sample Mean (MLE):{np.mean(Y):.2f}\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n```\nThe log-likelihood curve above confirms the theoretical result that the **MLE for a Poisson distribution is the sample mean** of the data. Here, the MLE is approximately **3.68**, indicated by the vertical red dashed line. This is the value of $\\lambda$ that **maximizes the likelihood** of observing our dataset under the Poisson model.\n\n\n#### Deriving the MLE for the Poisson Model\n\nWe start with the log-likelihood function for independent Poisson observations:\n\n$$\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n$$\n\nTo find the MLE, we take the derivative with respect to $\\lambda$:\n\n$$\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left[ -1 + \\frac{Y_i}{\\lambda} \\right] = -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n$$\n\nSetting the derivative equal to zero:\n\n$$\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n$$\n\nSolving for $\\lambda$:\n\n$$\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n$$\n\nThis result makes intuitive sense: for a Poisson distribution, the mean and the variance are both equal to $\\lambda$, so it's natural that the MLE is the sample mean.\n\n```{python}\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood\ndef neg_poisson_log_likelihood(lmbda, y):\n    # lmbda is passed as an array by minimize(), so take first element\n    if lmbda[0] <= 0:\n        return np.inf\n    return -poisson_log_likelihood(lmbda[0], y)\n\n# Initial guess and data\nY = blueprinty_data['patents'].values\ninitial_guess = [1.0]\n\n# Optimize\nresult = minimize(neg_poisson_log_likelihood, x0=initial_guess, args=(Y,), method='Nelder-Mead')\n\n# Extract MLE\nlambda_mle = result.x[0]\nlambda_mle\n```\nUsing `scipy.optimize.minimize()`, we numerically maximized the Poisson log-likelihood and obtained an MLE for λ of approximately **3.685**. This estimate is consistent with both our analytical derivation (where the MLE is the sample mean) and our earlier visualization of the log-likelihood function.\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n```{python}\nimport numpy as np\nfrom scipy.special import gammaln\nimport math\n\n# Poisson regression log-likelihood using safe math.exp\ndef poisson_regression_log_likelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=float)\n    Xb = X @ beta\n    Xb = np.clip(Xb, -20, 20)  # Prevent overflow\n    lambda_i = np.array([math.exp(val) for val in Xb])  # Use math.exp for robustness\n    return np.sum(-lambda_i + Y * Xb - gammaln(Y + 1))\n\n```\n\n```{python}\nimport pandas as pd\nfrom scipy.optimize import minimize\nimport numdifftools as nd\n\n# Load and clean data\ndf = pd.read_csv(\"blueprinty.csv\").dropna(subset=[\"patents\", \"age\", \"region\", \"iscustomer\"])\n\n# Create variables\ndf[\"age_centered\"] = df[\"age\"] - df[\"age\"].mean()\ndf[\"age_sq\"] = df[\"age_centered\"] ** 2\nregion_dummies = pd.get_dummies(df[\"region\"], prefix=\"region\", drop_first=True)\n\n# Construct design matrix\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_centered\", \"age_sq\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX_matrix = X.astype(float).values\nY = df[\"patents\"].astype(float).values\n\n# Optimization wrapper\ndef neg_log_likelihood_beta(beta):\n    return -poisson_regression_log_likelihood(beta, Y, X_matrix)\n\n# Estimate β via MLE\ninit_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(neg_log_likelihood_beta, init_beta, method=\"BFGS\")\nbeta_mle = result.x\n\n# Compute standard errors via numerical Hessian\nhessian_fun = nd.Hessian(neg_log_likelihood_beta)\nhessian_matrix = hessian_fun(beta_mle)\ncov_matrix = np.linalg.inv(hessian_matrix)\nse_beta = np.sqrt(np.diag(cov_matrix))\n\n# Present results\nresults_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Coefficient\": beta_mle,\n    \"Std. Error\": se_beta\n}).round(5)\n\nresults_table\n\n\n```\n```{python}\nimport statsmodels.api as sm\n\n# Drop intercept column and ensure all columns are float\nX_sm = sm.add_constant(X.drop(columns=\"intercept\").astype(float))\n\n# Make sure Y is also float\nY_sm = Y.astype(float)\n\n# Fit Poisson GLM\nmodel = sm.GLM(Y_sm, X_sm, family=sm.families.Poisson())\nglm_result = model.fit()\n\n# Show coefficient and standard error summary\nglm_result.summary2().tables[1].round(5)\n\n\n\n```\n\n#### Interpretation of Results\n\nWe estimated a Poisson regression model using two methods:\n\n1. **Manual MLE approach** using `scipy.optimize.minimize()` and a custom log-likelihood function\n2. **GLM Poisson model** using `statsmodels.GLM()` for a built-in estimation method\n\nBoth models included the same covariates:\n- Firm age (centered) and age squared\n- Customer status (`iscustomer`)\n- Region (with dummies for Northeast, Northwest, South, and Southwest)\n\n---\n\n#### **Key Findings (Common Across Both Methods)**\n\n- **Intercept (~1.34)**  \n  Baseline expected patent count is about `exp(1.34) ≈ 3.83` for a non-customer firm at average age in the base region.\n\n- **Age Effects**\n    - `age_centered`: negative and statistically significant  \n    - `age_sq`: negative and highly significant  \n    - These confirm a **concave relationship**, meaning patenting increases with age to a point, then declines.\n\n- **Customer Effect**  \n    - `iscustomer` has a coefficient of ~**0.208** in both models  \n    - This translates to about a **23% increase** in expected patents for Blueprinty customers  \n    (`exp(0.208) ≈ 1.23`)\n\n- **Region Dummies**\n    - None of the region effects were statistically significant  \n    - This suggests geographic region has **no meaningful influence** on patent counts once age and customer status are accounted for\n\n#### **Conclusion**\nBlueprinty customers tend to have more patents, even after adjusting for age and region. This effect is both **statistically significant** and **economically meaningful**. The concave age effect suggests mid-aged firms are the most patent-productive. Geographic region shows no strong effect, indicating that Blueprinty’s impact is consistent across locations.\n\n\n\n\n```{python}\n# Make two versions of the design matrix:\n# X_0: all firms set to iscustomer = 0\n# X_1: all firms set to iscustomer = 1\n\nX_0 = X.copy()\nX_1 = X.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Convert to numpy arrays\nX_0_matrix = X_0.astype(float).values\nX_1_matrix = X_1.astype(float).values\n\n# Predicted patent counts using the estimated beta_mle\nXb_0 = X_0_matrix @ beta_mle\nXb_1 = X_1_matrix @ beta_mle\n\nXb_0 = np.clip(Xb_0, -20, 20)\nXb_1 = np.clip(Xb_1, -20, 20)\n\ny_pred_0 = np.exp(Xb_0)   # expected patents for non-customers\ny_pred_1 = np.exp(Xb_1)   # expected patents for customers\n\n# Difference in predicted patents\ndiff = y_pred_1 - y_pred_0\naverage_diff = np.mean(diff)\n\nprint(\"The average difference in expected patent count\",average_diff)\n\n```\n\nTo assess the practical impact of Blueprinty's software, we predicted the number of patents for each firm twice:\n    - \"Once assuming all firms were non-customers `(iscustomer = 0)`\"\n    - \"Once assuming all firms were customers `(iscustomer = 1)`\"\n\nHolding all other variables constant, the average increase in expected patent count from being a customer was approximately **0.79** patents per firm.\n\nThis finding provides strong support for Blueprinty’s marketing claim: their software is associated with a meaningful improvement in patenting outcomes, even after adjusting for firm age and regional effects.\n\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n```{python}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load data\ndf = pd.read_csv(\"airbnb.csv\")\n\n# Select variables of interest\ncols = [\n    \"number_of_reviews\", \"days\", \"room_type\", \"bathrooms\", \"bedrooms\",\n    \"price\", \"review_scores_cleanliness\", \"review_scores_location\",\n    \"review_scores_value\", \"instant_bookable\"\n]\ndf = df[cols]\n\n# Summary statistics\ndf.describe(include='all')\n\n# Histograms of numeric variables\nnumeric_cols = df.select_dtypes(include=['number', 'float', 'int']).columns\ndf[numeric_cols].hist(bins=30, figsize=(8, 6))\nplt.suptitle(\"Distributions of Numeric Variables\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n# Boxplot: reviews by room type\nsns.boxplot(x=\"room_type\", y=\"number_of_reviews\", data=df)\nplt.title(\"Number of Reviews by Room Type\")\nplt.show()\n\n# Reviews vs. price scatter\nsns.scatterplot(x=\"price\", y=\"number_of_reviews\", data=df)\nplt.xlim(0, 500)  # cap x-axis to reduce noise\nplt.title(\"Number of Reviews vs. Price\")\nplt.show()\n\n```\n\n### Exploratory Data Analysis\n\nWe explore the distribution and relationships between variables that may drive the number of reviews (a proxy for bookings) on Airbnb.\n\n#### Distributions of Numeric Variables\n\nThe histograms show that:\n- `number_of_reviews`, `price`, `days`, and review scores are all **right-skewed**.\n- Most listings have **low review counts**, but a few have over 300–400.\n- Listings are typically **priced under $200/night**, though some exceed $1,000.\n- Review scores for cleanliness, location, and value are **clustered near 9–10**, indicating positive feedback overall.\n- Most listings have **1–2 bedrooms and bathrooms**, with rare larger listings.\n\n#### Number of Reviews by Room Type\n\nA boxplot reveals:\n- **Entire homes** and **private rooms** tend to receive more reviews than **shared rooms**.\n- Shared rooms have a **lower median** and more **concentrated distribution** of review counts.\n- All room types have substantial **outliers** — listings with extremely high numbers of reviews.\n\n#### Number of Reviews vs. Price\n\nThe scatterplot of reviews vs. price shows a **negative trend**:\n- Listings priced **under $100** tend to have **more reviews**.\n- As price increases, the number of reviews generally **decreases**.\n- There is **high variance** among cheaper listings, suggesting other factors (e.g. cleanliness, location) influence popularity.\n\nTogether, these visuals suggest that **lower-priced**, **clean**, and **moderately-sized** listings in **popular formats (entire homes, private rooms)** tend to receive more reviews — supporting their inclusion in the Poisson regression model.\n\n```{python}\n# Drop rows with missing values in any of the relevant columns\ndf = df.dropna(subset=cols)\n\n# Reconfirm dataset shape and missing status\nprint(\"Shape after dropna:\", df.shape)\nprint(\"Missing values:\\n\", df.isnull().sum())\n\n```\n```{python}\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# 3. Encode instant_bookable and room_type\ndf[\"instant_bookable\"] = (df[\"instant_bookable\"] == \"t\").astype(int)\nroom_dummies = pd.get_dummies(df[\"room_type\"], prefix=\"room\", drop_first=True).astype(float)\n\n# 4. Build design matrix\nX = pd.concat([\n    df[[\"days\", \"bathrooms\", \"bedrooms\", \"price\",\n        \"review_scores_cleanliness\", \"review_scores_location\",\n        \"review_scores_value\", \"instant_bookable\"]],\n    room_dummies\n], axis=1)\n\n# 5. Convert all columns to float\nX = X.astype(float)\nX = sm.add_constant(X)\n\n# 6. Target variable\nY = df[\"number_of_reviews\"].astype(float)\n\n# 7. Drop any remaining rows with missing values\nvalid = X.notnull().all(axis=1) & Y.notnull()\nX_clean = X.loc[valid]\nY_clean = Y.loc[valid]\n\n# 8. Fit Poisson model\nmodel = sm.GLM(Y_clean, X_clean, family=sm.families.Poisson())\nglm_result = model.fit()\n\n# 9. Show results\nglm_result.summary2().tables[1].round(4)\n\n```\n\n### Interpretation of Model Coefficients\n\nThis Poisson regression models the number of **reviews** (as a proxy for bookings) as a function of listing characteristics, review scores, room type, and booking options. Coefficients represent changes in the **log of the expected number of reviews** for a one-unit increase in each variable.\n\n- **Intercept (`const = 3.65`)**  \n  The baseline expected number of reviews for a listing with all numeric features at 0 and in the reference room type (likely “Entire home/apt”) is approximately `exp(3.65) ≈ 38.3`.\n\n- **`days` (0.0000, p < 0.001)**  \n  Listings that have been available longer tend to accumulate more reviews. Though the coefficient is small, it is highly statistically significant.\n\n- **`bathrooms` (-0.1105, p < 0.001)**  \n  Listings with more bathrooms receive fewer reviews, possibly reflecting higher prices or listing types that are less frequently booked.\n\n- **`bedrooms` (+0.0756, p < 0.001)**  \n  More bedrooms are associated with more reviews, suggesting that larger listings attract more bookings.\n\n- **`price` (-0.0000, p < 0.001)**  \n  Price has a small but statistically significant negative effect. More expensive listings are slightly less likely to be booked.\n\n- **`review_scores_cleanliness` (+0.1138, p < 0.001)**  \n  Cleanliness is a strong positive predictor of reviews, highlighting its importance in guest satisfaction and booking likelihood.\n\n- **`review_scores_location` (-0.0809, p < 0.001)**  \n  Surprisingly, higher location scores are associated with slightly fewer reviews, possibly due to correlation with other unobserved variables like price or room size.\n\n- **`review_scores_value` (-0.0971, p < 0.001)**  \n  Similarly, higher value scores show a negative association with reviews, perhaps reflecting that lower-priced listings (which tend to get high \"value\" ratings) have more reviews early on, but level off.\n\n- **`instant_bookable` (+0.0000, p < 0.001)**  \n  The effect size is near zero, but significant. Instant booking may have a marginal effect on bookings, or the signal may be too weak in the presence of stronger predictors.\n\n- **`room_Private room` (+0.0121, p < 0.001)**  \n  Private rooms receive slightly more reviews than entire homes, possibly due to affordability or volume.\n\n- **`room_Shared room` (-0.2172, p < 0.001)**  \n  Shared rooms are significantly less reviewed, suggesting lower demand.\n\n### Summary\n\nThe results highlight that **cleanliness**, **number of bedrooms**, and **length of listing availability** are important positive predictors of bookings, while **shared rooms**, **higher prices**, and some review scores (e.g., value, location) are associated with fewer reviews. The impact of `instant_bookable` is statistically significant but practically small.\n\n\n\n\n","srcMarkdownNoYaml":"\n\n\n## Blueprinty Case Study\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n\n```{python}\nimport pandas as pd\nblueprinty_data = pd.read_csv(\"blueprinty.csv\")\nblueprinty_data.head()\n```\n\n```{python}\n# Compute mean number of patents for customers and non-customers\nmean_patents = blueprinty_data.groupby('iscustomer')['patents'].mean()\nmean_patents\n```\n\n```{python}\nimport matplotlib.pyplot as plt\n# Bin the number of patents into intervals of size 2\nblueprinty_data['patent_bin'] = pd.cut(\n    blueprinty_data['patents'],\n    bins=range(0, blueprinty_data['patents'].max() + 2, 2),\n    right=False\n)\n\n# Count number of firms in each bin by customer status\npatent_counts = blueprinty_data.groupby(['patent_bin', 'iscustomer']).size().unstack(fill_value=0)\n\n# Convert to row-wise proportions\npatent_props = patent_counts.div(patent_counts.sum(axis=1), axis=0)\n\n# Plot\npatent_props.plot(kind='bar', figsize=(8, 6), width=0.8, color=['#efc0fc', '#c0e861'])\nplt.title('Proportion of Firms by Patent Bin and Customer Status')\nplt.xlabel('Number of Patents (Binned)')\nplt.ylabel('Proportion of Firms')\nplt.xticks(rotation=45)\nplt.legend(title='Is Customer', labels=['No', 'Yes'])\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n```\n\n\nWe observe that **Blueprinty customers tend to have more patents** than non-customers, both on average and in their distribution.\n\n- The **average number of patents** is higher for customers (**4.13**) than for non-customers (**3.47**).\n- **Non-customers** are heavily concentrated in lower patent bins (e.g., `0–6` patents).\n- **Customers** appear more frequently in **higher patent bins**, such as `6–10` and `10–14`.\n- When proportions are normalized by group size, **customers are overrepresented** in higher patent brackets. In the `[12–14)` bin, customers even make up the majority.\n\nThis pattern suggests that firms using Blueprinty's software are **more successful in obtaining patents**, supporting the hypothesis that Blueprinty's product contributes to improved patenting outcomes.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n```{python}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Boxplot of age by customer status\nplt.figure(figsize=(7, 5))\nsns.boxplot(x='iscustomer', y='age', data=blueprinty_data, palette=[\"#efc0fc\", \"#c3ff63\"])\nplt.title(\"Firm Age by Blueprinty Customer Status\")\nplt.xlabel(\"Is Customer (0 = No, 1 = Yes)\")\nplt.ylabel(\"Firm Age (Years)\")\nplt.grid(True)\nplt.show()\n```\n#### Firm Age\n\nThe boxplot comparing firm ages by Blueprinty customer status reveals that **customers tend to be slightly older on average** than non-customers. \n\n- While the **median age** is somewhat higher for customers, the overall spread (**interquartile range**) is similar across both groups.  \n- This suggests that **more established firms** may be more likely to adopt Blueprinty's software.\n\n\n```{python}\n# Crosstab of customer status by region\nregion_counts = pd.crosstab(blueprinty_data['region'], blueprinty_data['iscustomer'])\n\n# Convert to proportions (row-wise)\nregion_props = region_counts.div(region_counts.sum(axis=1), axis=0)\n\n# Bar plot of customer proportions by region\nregion_props.plot(kind='bar', stacked=True, figsize=(8, 5), colormap='Set2')\nplt.title(\"Proportion of Customers by Region\")\nplt.ylabel(\"Proportion of Firms\")\nplt.xlabel(\"Region\")\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nregion_counts\n\n```\n#### Region\nThe stacked bar chart shows variation in Blueprinty adoption rates across regions:\n\n- The **Northeast** has a noticeably higher proportion of customers, with over **50%** of firms using Blueprinty.\n- In contrast, other regions like the **Midwest**, **Northwest**, **South**, and **Southwest** have much lower customer proportions — around **15–20%**.\n\nThis suggests stronger **regional presence** or **marketing penetration** in the Northeast, which could influence future outreach or sales strategy.\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nWe assume that the number of patents $Y_i$ for firm $i$ follows a Poisson distribution:\n\n$$\nY_i \\sim \\text{Poisson}(\\lambda)\n$$\n\nThe likelihood for a single observation is:\n\n$$\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nAnd the joint likelihood over $n$ independent firms is:\n\n$$\nL(\\lambda \\mid Y_1, \\dots, Y_n) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n$$\n\nTaking logs gives the log-likelihood function:\n\n$$\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n$$\n\n\n```{python}\nimport numpy as np\nfrom scipy.special import factorial\n\n# Define Poisson log-likelihood function\ndef poisson_log_likelihood(lmbda, y):\n    if lmbda <= 0:\n        return -np.inf  # log-likelihood undefined for non-positive lambda\n    return np.sum(y * np.log(lmbda) - lmbda - np.log(factorial(y)))\n\nY = blueprinty_data['patents'].values\npoisson_log_likelihood(lmbda=4.0, y=Y)  # Example evaluation at lambda = 4.0\n\n```\n\nThis number by itself isn’t “good” or “bad” — it's just **one point on the log-likelihood curve**. The goal now is to find the value of $\\lambda$ that **maximizes this log-likelihood** — that is, the **Maximum Likelihood Estimate (MLE)** of $\\lambda$.\n\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Observed patent data\nY = blueprinty_data['patents'].values\n\n# Lambda range to evaluate\nlambda_vals = np.linspace(0.1, 10, 200)\nloglik_vals = [poisson_log_likelihood(lmbda, Y) for lmbda in lambda_vals]\n\n# Plot log-likelihood curve\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_vals, loglik_vals, color='green')\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.axvline(x=np.mean(Y), color='red', linestyle='--', label=f\"Sample Mean (MLE):{np.mean(Y):.2f}\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n```\nThe log-likelihood curve above confirms the theoretical result that the **MLE for a Poisson distribution is the sample mean** of the data. Here, the MLE is approximately **3.68**, indicated by the vertical red dashed line. This is the value of $\\lambda$ that **maximizes the likelihood** of observing our dataset under the Poisson model.\n\n\n#### Deriving the MLE for the Poisson Model\n\nWe start with the log-likelihood function for independent Poisson observations:\n\n$$\n\\ell(\\lambda) = \\sum_{i=1}^n \\left[ -\\lambda + Y_i \\log(\\lambda) - \\log(Y_i!) \\right]\n$$\n\nTo find the MLE, we take the derivative with respect to $\\lambda$:\n\n$$\n\\frac{d\\ell}{d\\lambda} = \\sum_{i=1}^n \\left[ -1 + \\frac{Y_i}{\\lambda} \\right] = -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n$$\n\nSetting the derivative equal to zero:\n\n$$\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n$$\n\nSolving for $\\lambda$:\n\n$$\n\\hat{\\lambda}_{\\text{MLE}} = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n$$\n\nThis result makes intuitive sense: for a Poisson distribution, the mean and the variance are both equal to $\\lambda$, so it's natural that the MLE is the sample mean.\n\n```{python}\nfrom scipy.optimize import minimize\n\n# Define the negative log-likelihood\ndef neg_poisson_log_likelihood(lmbda, y):\n    # lmbda is passed as an array by minimize(), so take first element\n    if lmbda[0] <= 0:\n        return np.inf\n    return -poisson_log_likelihood(lmbda[0], y)\n\n# Initial guess and data\nY = blueprinty_data['patents'].values\ninitial_guess = [1.0]\n\n# Optimize\nresult = minimize(neg_poisson_log_likelihood, x0=initial_guess, args=(Y,), method='Nelder-Mead')\n\n# Extract MLE\nlambda_mle = result.x[0]\nlambda_mle\n```\nUsing `scipy.optimize.minimize()`, we numerically maximized the Poisson log-likelihood and obtained an MLE for λ of approximately **3.685**. This estimate is consistent with both our analytical derivation (where the MLE is the sample mean) and our earlier visualization of the log-likelihood function.\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n```{python}\nimport numpy as np\nfrom scipy.special import gammaln\nimport math\n\n# Poisson regression log-likelihood using safe math.exp\ndef poisson_regression_log_likelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=float)\n    Xb = X @ beta\n    Xb = np.clip(Xb, -20, 20)  # Prevent overflow\n    lambda_i = np.array([math.exp(val) for val in Xb])  # Use math.exp for robustness\n    return np.sum(-lambda_i + Y * Xb - gammaln(Y + 1))\n\n```\n\n```{python}\nimport pandas as pd\nfrom scipy.optimize import minimize\nimport numdifftools as nd\n\n# Load and clean data\ndf = pd.read_csv(\"blueprinty.csv\").dropna(subset=[\"patents\", \"age\", \"region\", \"iscustomer\"])\n\n# Create variables\ndf[\"age_centered\"] = df[\"age\"] - df[\"age\"].mean()\ndf[\"age_sq\"] = df[\"age_centered\"] ** 2\nregion_dummies = pd.get_dummies(df[\"region\"], prefix=\"region\", drop_first=True)\n\n# Construct design matrix\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[[\"age_centered\", \"age_sq\", \"iscustomer\"]],\n    region_dummies\n], axis=1)\nX_matrix = X.astype(float).values\nY = df[\"patents\"].astype(float).values\n\n# Optimization wrapper\ndef neg_log_likelihood_beta(beta):\n    return -poisson_regression_log_likelihood(beta, Y, X_matrix)\n\n# Estimate β via MLE\ninit_beta = np.zeros(X_matrix.shape[1])\nresult = minimize(neg_log_likelihood_beta, init_beta, method=\"BFGS\")\nbeta_mle = result.x\n\n# Compute standard errors via numerical Hessian\nhessian_fun = nd.Hessian(neg_log_likelihood_beta)\nhessian_matrix = hessian_fun(beta_mle)\ncov_matrix = np.linalg.inv(hessian_matrix)\nse_beta = np.sqrt(np.diag(cov_matrix))\n\n# Present results\nresults_table = pd.DataFrame({\n    \"Variable\": X.columns,\n    \"Coefficient\": beta_mle,\n    \"Std. Error\": se_beta\n}).round(5)\n\nresults_table\n\n\n```\n```{python}\nimport statsmodels.api as sm\n\n# Drop intercept column and ensure all columns are float\nX_sm = sm.add_constant(X.drop(columns=\"intercept\").astype(float))\n\n# Make sure Y is also float\nY_sm = Y.astype(float)\n\n# Fit Poisson GLM\nmodel = sm.GLM(Y_sm, X_sm, family=sm.families.Poisson())\nglm_result = model.fit()\n\n# Show coefficient and standard error summary\nglm_result.summary2().tables[1].round(5)\n\n\n\n```\n\n#### Interpretation of Results\n\nWe estimated a Poisson regression model using two methods:\n\n1. **Manual MLE approach** using `scipy.optimize.minimize()` and a custom log-likelihood function\n2. **GLM Poisson model** using `statsmodels.GLM()` for a built-in estimation method\n\nBoth models included the same covariates:\n- Firm age (centered) and age squared\n- Customer status (`iscustomer`)\n- Region (with dummies for Northeast, Northwest, South, and Southwest)\n\n---\n\n#### **Key Findings (Common Across Both Methods)**\n\n- **Intercept (~1.34)**  \n  Baseline expected patent count is about `exp(1.34) ≈ 3.83` for a non-customer firm at average age in the base region.\n\n- **Age Effects**\n    - `age_centered`: negative and statistically significant  \n    - `age_sq`: negative and highly significant  \n    - These confirm a **concave relationship**, meaning patenting increases with age to a point, then declines.\n\n- **Customer Effect**  \n    - `iscustomer` has a coefficient of ~**0.208** in both models  \n    - This translates to about a **23% increase** in expected patents for Blueprinty customers  \n    (`exp(0.208) ≈ 1.23`)\n\n- **Region Dummies**\n    - None of the region effects were statistically significant  \n    - This suggests geographic region has **no meaningful influence** on patent counts once age and customer status are accounted for\n\n#### **Conclusion**\nBlueprinty customers tend to have more patents, even after adjusting for age and region. This effect is both **statistically significant** and **economically meaningful**. The concave age effect suggests mid-aged firms are the most patent-productive. Geographic region shows no strong effect, indicating that Blueprinty’s impact is consistent across locations.\n\n\n\n\n```{python}\n# Make two versions of the design matrix:\n# X_0: all firms set to iscustomer = 0\n# X_1: all firms set to iscustomer = 1\n\nX_0 = X.copy()\nX_1 = X.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Convert to numpy arrays\nX_0_matrix = X_0.astype(float).values\nX_1_matrix = X_1.astype(float).values\n\n# Predicted patent counts using the estimated beta_mle\nXb_0 = X_0_matrix @ beta_mle\nXb_1 = X_1_matrix @ beta_mle\n\nXb_0 = np.clip(Xb_0, -20, 20)\nXb_1 = np.clip(Xb_1, -20, 20)\n\ny_pred_0 = np.exp(Xb_0)   # expected patents for non-customers\ny_pred_1 = np.exp(Xb_1)   # expected patents for customers\n\n# Difference in predicted patents\ndiff = y_pred_1 - y_pred_0\naverage_diff = np.mean(diff)\n\nprint(\"The average difference in expected patent count\",average_diff)\n\n```\n\nTo assess the practical impact of Blueprinty's software, we predicted the number of patents for each firm twice:\n    - \"Once assuming all firms were non-customers `(iscustomer = 0)`\"\n    - \"Once assuming all firms were customers `(iscustomer = 1)`\"\n\nHolding all other variables constant, the average increase in expected patent count from being a customer was approximately **0.79** patents per firm.\n\nThis finding provides strong support for Blueprinty’s marketing claim: their software is associated with a meaningful improvement in patenting outcomes, even after adjusting for firm age and regional effects.\n\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n```{python}\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load data\ndf = pd.read_csv(\"airbnb.csv\")\n\n# Select variables of interest\ncols = [\n    \"number_of_reviews\", \"days\", \"room_type\", \"bathrooms\", \"bedrooms\",\n    \"price\", \"review_scores_cleanliness\", \"review_scores_location\",\n    \"review_scores_value\", \"instant_bookable\"\n]\ndf = df[cols]\n\n# Summary statistics\ndf.describe(include='all')\n\n# Histograms of numeric variables\nnumeric_cols = df.select_dtypes(include=['number', 'float', 'int']).columns\ndf[numeric_cols].hist(bins=30, figsize=(8, 6))\nplt.suptitle(\"Distributions of Numeric Variables\", fontsize=16)\nplt.tight_layout()\nplt.show()\n\n# Boxplot: reviews by room type\nsns.boxplot(x=\"room_type\", y=\"number_of_reviews\", data=df)\nplt.title(\"Number of Reviews by Room Type\")\nplt.show()\n\n# Reviews vs. price scatter\nsns.scatterplot(x=\"price\", y=\"number_of_reviews\", data=df)\nplt.xlim(0, 500)  # cap x-axis to reduce noise\nplt.title(\"Number of Reviews vs. Price\")\nplt.show()\n\n```\n\n### Exploratory Data Analysis\n\nWe explore the distribution and relationships between variables that may drive the number of reviews (a proxy for bookings) on Airbnb.\n\n#### Distributions of Numeric Variables\n\nThe histograms show that:\n- `number_of_reviews`, `price`, `days`, and review scores are all **right-skewed**.\n- Most listings have **low review counts**, but a few have over 300–400.\n- Listings are typically **priced under $200/night**, though some exceed $1,000.\n- Review scores for cleanliness, location, and value are **clustered near 9–10**, indicating positive feedback overall.\n- Most listings have **1–2 bedrooms and bathrooms**, with rare larger listings.\n\n#### Number of Reviews by Room Type\n\nA boxplot reveals:\n- **Entire homes** and **private rooms** tend to receive more reviews than **shared rooms**.\n- Shared rooms have a **lower median** and more **concentrated distribution** of review counts.\n- All room types have substantial **outliers** — listings with extremely high numbers of reviews.\n\n#### Number of Reviews vs. Price\n\nThe scatterplot of reviews vs. price shows a **negative trend**:\n- Listings priced **under $100** tend to have **more reviews**.\n- As price increases, the number of reviews generally **decreases**.\n- There is **high variance** among cheaper listings, suggesting other factors (e.g. cleanliness, location) influence popularity.\n\nTogether, these visuals suggest that **lower-priced**, **clean**, and **moderately-sized** listings in **popular formats (entire homes, private rooms)** tend to receive more reviews — supporting their inclusion in the Poisson regression model.\n\n```{python}\n# Drop rows with missing values in any of the relevant columns\ndf = df.dropna(subset=cols)\n\n# Reconfirm dataset shape and missing status\nprint(\"Shape after dropna:\", df.shape)\nprint(\"Missing values:\\n\", df.isnull().sum())\n\n```\n```{python}\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# 3. Encode instant_bookable and room_type\ndf[\"instant_bookable\"] = (df[\"instant_bookable\"] == \"t\").astype(int)\nroom_dummies = pd.get_dummies(df[\"room_type\"], prefix=\"room\", drop_first=True).astype(float)\n\n# 4. Build design matrix\nX = pd.concat([\n    df[[\"days\", \"bathrooms\", \"bedrooms\", \"price\",\n        \"review_scores_cleanliness\", \"review_scores_location\",\n        \"review_scores_value\", \"instant_bookable\"]],\n    room_dummies\n], axis=1)\n\n# 5. Convert all columns to float\nX = X.astype(float)\nX = sm.add_constant(X)\n\n# 6. Target variable\nY = df[\"number_of_reviews\"].astype(float)\n\n# 7. Drop any remaining rows with missing values\nvalid = X.notnull().all(axis=1) & Y.notnull()\nX_clean = X.loc[valid]\nY_clean = Y.loc[valid]\n\n# 8. Fit Poisson model\nmodel = sm.GLM(Y_clean, X_clean, family=sm.families.Poisson())\nglm_result = model.fit()\n\n# 9. Show results\nglm_result.summary2().tables[1].round(4)\n\n```\n\n### Interpretation of Model Coefficients\n\nThis Poisson regression models the number of **reviews** (as a proxy for bookings) as a function of listing characteristics, review scores, room type, and booking options. Coefficients represent changes in the **log of the expected number of reviews** for a one-unit increase in each variable.\n\n- **Intercept (`const = 3.65`)**  \n  The baseline expected number of reviews for a listing with all numeric features at 0 and in the reference room type (likely “Entire home/apt”) is approximately `exp(3.65) ≈ 38.3`.\n\n- **`days` (0.0000, p < 0.001)**  \n  Listings that have been available longer tend to accumulate more reviews. Though the coefficient is small, it is highly statistically significant.\n\n- **`bathrooms` (-0.1105, p < 0.001)**  \n  Listings with more bathrooms receive fewer reviews, possibly reflecting higher prices or listing types that are less frequently booked.\n\n- **`bedrooms` (+0.0756, p < 0.001)**  \n  More bedrooms are associated with more reviews, suggesting that larger listings attract more bookings.\n\n- **`price` (-0.0000, p < 0.001)**  \n  Price has a small but statistically significant negative effect. More expensive listings are slightly less likely to be booked.\n\n- **`review_scores_cleanliness` (+0.1138, p < 0.001)**  \n  Cleanliness is a strong positive predictor of reviews, highlighting its importance in guest satisfaction and booking likelihood.\n\n- **`review_scores_location` (-0.0809, p < 0.001)**  \n  Surprisingly, higher location scores are associated with slightly fewer reviews, possibly due to correlation with other unobserved variables like price or room size.\n\n- **`review_scores_value` (-0.0971, p < 0.001)**  \n  Similarly, higher value scores show a negative association with reviews, perhaps reflecting that lower-priced listings (which tend to get high \"value\" ratings) have more reviews early on, but level off.\n\n- **`instant_bookable` (+0.0000, p < 0.001)**  \n  The effect size is near zero, but significant. Instant booking may have a marginal effect on bookings, or the signal may be too weak in the presence of stronger predictors.\n\n- **`room_Private room` (+0.0121, p < 0.001)**  \n  Private rooms receive slightly more reviews than entire homes, possibly due to affordability or volume.\n\n- **`room_Shared room` (-0.2172, p < 0.001)**  \n  Shared rooms are significantly less reviewed, suggesting lower demand.\n\n### Summary\n\nThe results highlight that **cleanliness**, **number of bedrooms**, and **length of listing availability** are important positive predictors of bookings, while **shared rooms**, **higher prices**, and some review scores (e.g., value, location) are associated with fewer reviews. The impact of `instant_bookable` is statistically significant but practically small.\n\n\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.5","theme":["cosmo","brand"],"title":"Poisson Regression Examples","author":"Samiksha Shetty","date":"today","callout-appearance":"minimal"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}